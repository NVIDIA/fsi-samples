{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T01:40:19.022807Z",
     "start_time": "2021-10-12T01:40:19.007166Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##\n",
    "## Copyright (C) 2021 NVIDIA Corporation.  All rights reserved.\n",
    "##\n",
    "## NVIDIA Sample Code\n",
    "##\n",
    "## Please refer to the NVIDIA end user license agreement (EULA) associated\n",
    "## with this source code for terms and conditions that govern your use of\n",
    "## this software. Any use, reproduction, disclosure, or distribution of\n",
    "## this software and related documentation outside the terms of the EULA\n",
    "## is strictly prohibited.\n",
    "##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using backend: pytorch\n"
     ]
    }
   ],
   "source": [
    "import cudf\n",
    "import dgl\n",
    "from dgl.data import DGLDataset\n",
    "import torch\n",
    "import os\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = {\n",
    "    'tx_id': \"int32\",\n",
    "    'timestamp': \"int32\",\n",
    "    'sender_account_id': \"int32\",\n",
    "    'receiver_account_id': \"int32\",\n",
    "    'tx_amount': \"float32\",\n",
    "    'sender_init_balance': \"float32\",\n",
    "    'receiver_init_balance': \"float32\",\n",
    "    'alert_id': \"int32\",\n",
    "    'sender_is_fraud': \"int32\",\n",
    "    'receiver_is_fraud': \"int32\",\n",
    "    'is_fraud': \"int32\",\n",
    "    'sender_fin_balance': \"float32\",\n",
    "    'receiver_fin_balance': \"float32\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transactions():\n",
    "    df0 = cudf.read_csv('1m_cleaned/fixed_transactions_part1.csv', dtype=types)\n",
    "    df1 = cudf.read_csv('1m_cleaned/fixed_transactions_part2.csv', dtype=types)\n",
    "    df2 = cudf.read_csv('1m_cleaned/fixed_transactions_part3.csv', dtype=types)\n",
    "    return cudf.concat([df0, df1, df2])\n",
    "    #return cudf.concat([df0, df1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMLDataset(DGLDataset):\n",
    "    def __init__(self):\n",
    "        super().__init__(name='aml')\n",
    "\n",
    "    def process(self):\n",
    "        df = load_transactions()\n",
    "        self.graph = g = dgl.graph((df.sender_account_id, df.receiver_account_id), num_nodes=df.sender_account_id.nunique(), \n",
    "                                   device=device)\n",
    "        self.graph.edata['label'] = torch.cuda.FloatTensor(df.is_fraud.values)\n",
    "        \n",
    "        ins = self.graph.in_degrees()\n",
    "        outs = self.graph.out_degrees()        \n",
    "        self.graph.ndata['feat'] = torch.vstack([ins, outs]).permute([1, 0]).type(torch.float32)\n",
    "        \n",
    "        deg_means = self.graph.ndata['feat'].mean(axis=0)\n",
    "        deg_stds = self.graph.ndata['feat'].std(axis=0)\n",
    "        self.graph.ndata['feat'] = (self.graph.ndata['feat'] - deg_means) / deg_stds \n",
    "\n",
    "        mean_val = df['tx_amount'].mean()\n",
    "        std = df['tx_amount'].std()\n",
    "        self.graph.edata['feat'] = torch.cuda.FloatTensor((df['tx_amount'] - mean_val) / std)\n",
    "               \n",
    "        number_edges = self.graph.num_edges()\n",
    "        n_train = int(number_edges * 0.6)\n",
    "        n_val = int(number_edges * 0.2)\n",
    "        train_mask = torch.zeros(number_edges, dtype=torch.bool, device=device)\n",
    "        val_mask = torch.zeros(number_edges, dtype=torch.bool, device=device)\n",
    "        test_mask = torch.zeros(number_edges, dtype=torch.bool, device=device)\n",
    "        train_mask[:n_train] = True\n",
    "        val_mask[n_train:n_train + n_val] = True\n",
    "        test_mask[n_train + n_val:] = True\n",
    "        self.graph.edata['train_mask'] = train_mask\n",
    "        self.graph.edata['val_mask'] = val_mask\n",
    "        self.graph.edata['test_mask'] = test_mask\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "\n",
    "class SAGEConv(nn.Module):\n",
    "    \"\"\"Graph convolution module used by the GraphSAGE model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_feat : int\n",
    "        Input feature size.\n",
    "    out_feat : int\n",
    "        Output feature size.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_feat, out_feat):\n",
    "        super(SAGEConv, self).__init__()\n",
    "        # A linear submodule for projecting the input and neighbor feature to the output.\n",
    "        self.linear = nn.Linear(in_feat * 2, out_feat)\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        \"\"\"Forward computation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : Graph\n",
    "            The input graph.\n",
    "        h : Tensor\n",
    "            The input node feature.\n",
    "        \"\"\"\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # update_all is a message passing API.\n",
    "            g.update_all(message_func=dgl.function.e_mul_u('feat', 'h', 'm'), reduce_func=fn.mean('m', 'h_N'))\n",
    "            h_N = g.ndata['h_N']\n",
    "            h_total = torch.cat([h, h_N], dim=1)\n",
    "            return self.linear(h_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotPredictor(nn.Module):\n",
    "    def forward(self, g, h):\n",
    "        with g.local_scope():\n",
    "            g.ndata['h'] = h\n",
    "            # Compute a new edge feature named 'score' by a dot-product between the\n",
    "            # source node feature 'h' and destination node feature 'h'.\n",
    "            g.apply_edges(fn.u_dot_v('h', 'h', 'score'))\n",
    "            # u_dot_v returns a 1-element vector for each edge so you need to squeeze it.\n",
    "            return g.edata['score'][:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_feats, h_feats, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats)\n",
    "        self.conv2 = SAGEConv(h_feats, num_classes)\n",
    "\n",
    "    def forward(self, g, in_feat):\n",
    "        h = self.conv1(g, in_feat)\n",
    "        h = F.relu(h)\n",
    "        h = self.conv2(g, h)\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = AMLDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.AMLDataset at 0x2b931a5167c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=1000000, num_edges=124703184,\n",
       "      ndata_schemes={'feat': Scheme(shape=(2,), dtype=torch.float32)}\n",
       "      edata_schemes={'label': Scheme(shape=(), dtype=torch.float32), 'feat': Scheme(shape=(), dtype=torch.float32), 'train_mask': Scheme(shape=(), dtype=torch.bool), 'val_mask': Scheme(shape=(), dtype=torch.bool), 'test_mask': Scheme(shape=(), dtype=torch.bool)})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In epoch 0, loss: 0.702, val acc: 0.198 (best 0.198), test acc: 0.198 (best 0.198)\n",
      "In epoch 50, loss: 0.625, val acc: 0.652 (best 0.652), test acc: 0.652 (best 0.652)\n",
      "In epoch 100, loss: 0.598, val acc: 0.667 (best 0.667), test acc: 0.667 (best 0.667)\n",
      "In epoch 150, loss: 0.589, val acc: 0.677 (best 0.677), test acc: 0.677 (best 0.677)\n",
      "In epoch 200, loss: 0.580, val acc: 0.686 (best 0.686), test acc: 0.686 (best 0.686)\n",
      "In epoch 250, loss: 0.574, val acc: 0.691 (best 0.691), test acc: 0.691 (best 0.691)\n",
      "In epoch 300, loss: 0.569, val acc: 0.695 (best 0.695), test acc: 0.695 (best 0.696)\n",
      "In epoch 350, loss: 0.563, val acc: 0.700 (best 0.701), test acc: 0.701 (best 0.701)\n",
      "In epoch 400, loss: 0.559, val acc: 0.701 (best 0.701), test acc: 0.701 (best 0.701)\n",
      "In epoch 450, loss: 0.557, val acc: 0.701 (best 0.701), test acc: 0.701 (best 0.701)\n",
      "In epoch 500, loss: 0.555, val acc: 0.701 (best 0.701), test acc: 0.701 (best 0.701)\n",
      "In epoch 550, loss: 0.554, val acc: 0.701 (best 0.702), test acc: 0.701 (best 0.702)\n",
      "In epoch 600, loss: 0.552, val acc: 0.702 (best 0.702), test acc: 0.702 (best 0.702)\n",
      "In epoch 650, loss: 0.551, val acc: 0.703 (best 0.703), test acc: 0.703 (best 0.703)\n",
      "In epoch 700, loss: 0.550, val acc: 0.703 (best 0.704), test acc: 0.703 (best 0.704)\n",
      "In epoch 750, loss: 0.549, val acc: 0.704 (best 0.704), test acc: 0.704 (best 0.704)\n",
      "In epoch 800, loss: 0.549, val acc: 0.705 (best 0.705), test acc: 0.705 (best 0.705)\n",
      "In epoch 850, loss: 0.548, val acc: 0.705 (best 0.705), test acc: 0.705 (best 0.705)\n",
      "In epoch 900, loss: 0.547, val acc: 0.705 (best 0.705), test acc: 0.705 (best 0.705)\n",
      "In epoch 950, loss: 0.547, val acc: 0.705 (best 0.706), test acc: 0.705 (best 0.706)\n",
      "In epoch 1000, loss: 0.546, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1050, loss: 0.546, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1100, loss: 0.546, val acc: 0.705 (best 0.706), test acc: 0.705 (best 0.706)\n",
      "In epoch 1150, loss: 0.545, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1200, loss: 0.545, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1250, loss: 0.544, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1300, loss: 0.544, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1350, loss: 0.544, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1400, loss: 0.544, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1450, loss: 0.544, val acc: 0.706 (best 0.706), test acc: 0.706 (best 0.706)\n",
      "In epoch 1500, loss: 0.543, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1550, loss: 0.543, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1600, loss: 0.543, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1650, loss: 0.547, val acc: 0.703 (best 0.707), test acc: 0.703 (best 0.707)\n",
      "In epoch 1700, loss: 0.542, val acc: 0.707 (best 0.707), test acc: 0.707 (best 0.707)\n",
      "In epoch 1750, loss: 0.542, val acc: 0.707 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1800, loss: 0.542, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1850, loss: 0.542, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1900, loss: 0.543, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n",
      "In epoch 1950, loss: 0.542, val acc: 0.706 (best 0.707), test acc: 0.706 (best 0.707)\n"
     ]
    }
   ],
   "source": [
    "def train(g, model):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    #all_logits = []\n",
    "    best_val_acc = 0\n",
    "    best_test_acc = 0\n",
    "    \n",
    "    features = g.ndata['feat']\n",
    "    edge_label = g.edata['label'].type(torch.float32)\n",
    "    train_mask = g.edata['train_mask']\n",
    "    val_mask = g.edata['val_mask']\n",
    "    test_mask = g.edata['test_mask']\n",
    "    \n",
    "    score = DotPredictor().cuda()\n",
    "    opt = torch.optim.Adam(model.parameters())\n",
    "    for epoch in range(10000):\n",
    "        pred = model(g, features)\n",
    "        logits = score(g, pred)\n",
    "        loss = F.binary_cross_entropy_with_logits(logits[train_mask], edge_label[train_mask])\n",
    "        # Compute prediction\n",
    "        pred = (logits.detach()>0.0).type(torch.int32)\n",
    "\n",
    "        # Compute loss\n",
    "        # Note that we should only compute the losses of the nodes in the training set,\n",
    "        # i.e. with train_mask 1.\n",
    "       \n",
    "\n",
    "        # Compute accuracy on training/validation/test\n",
    "        train_acc = (pred[train_mask] == edge_label[train_mask]).float().mean()\n",
    "        val_acc = (pred[val_mask] == edge_label[val_mask]).float().mean()\n",
    "        test_acc = (pred[test_mask] == edge_label[test_mask]).float().mean()\n",
    "\n",
    "        # Save the best validation accuracy and the corresponding test accuracy.\n",
    "        if best_val_acc < val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_test_acc = test_acc\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #all_logits.append(logits.detach())\n",
    "\n",
    "        if epoch % 50 == 0:\n",
    "            print('In epoch {}, loss: {:.3f}, val acc: {:.3f} (best {:.3f}), test acc: {:.3f} (best {:.3f})'.format(\n",
    "                epoch, loss, val_acc, best_val_acc, test_acc, best_test_acc))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "model = Model(g.ndata['feat'].shape[1], 16, 2).to(device)\n",
    "train(g.to(device), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.7.1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
