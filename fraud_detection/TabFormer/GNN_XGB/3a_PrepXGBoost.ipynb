{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-12T01:45:19.942737Z",
     "start_time": "2021-10-12T01:45:19.926372Z"
    }
   },
   "outputs": [],
   "source": [
    "############################################################################\n",
    "##\n",
    "## Copyright (C) 2021 NVIDIA Corporation.  All rights reserved.\n",
    "##\n",
    "## NVIDIA Sample Code\n",
    "##\n",
    "## Please refer to the NVIDIA end user license agreement (EULA) associated\n",
    "## with this source code for terms and conditions that govern your use of\n",
    "## this software. Any use, reproduction, disclosure, or distribution of\n",
    "## this software and related documentation outside the terms of the EULA\n",
    "## is strictly prohibited.\n",
    "##\n",
    "############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we show how RAPIDS libraries like cuDF (GPU version of Pandas), cuML (GPU version of Scikit-learn) can be used to GPU-accelerated the data preprocessing and feature engineering needed for training XGBoost model on the TabFormer dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "import cudf\n",
    "import numpy as np\n",
    "\n",
    "import cupy as cp\n",
    "import cuml\n",
    "from cuml.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"./basedir\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define and clean our processed data\n",
    "processed_path = os.path.join(BASE_DIR, \"processed_data_1gpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = cudf.read_parquet(os.path.join(processed_path, 'subset_transactions.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>card</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>time</th>\n",
       "      <th>amount</th>\n",
       "      <th>use_chip</th>\n",
       "      <th>merchant_id</th>\n",
       "      <th>merchant_city</th>\n",
       "      <th>merchant_state</th>\n",
       "      <th>zip</th>\n",
       "      <th>mcc</th>\n",
       "      <th>errors</th>\n",
       "      <th>is_fraud</th>\n",
       "      <th>card_id</th>\n",
       "      <th>is_train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>06:21</td>\n",
       "      <td>$134.09</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>64518</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5300</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>06:42</td>\n",
       "      <td>$38.48</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>43058</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>06:22</td>\n",
       "      <td>$120.34</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>43058</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5411</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>17:45</td>\n",
       "      <td>$128.95</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>63932</td>\n",
       "      <td>Monterey Park</td>\n",
       "      <td>CA</td>\n",
       "      <td>91754.0</td>\n",
       "      <td>5651</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2002</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>06:23</td>\n",
       "      <td>$104.71</td>\n",
       "      <td>Swipe Transaction</td>\n",
       "      <td>76114</td>\n",
       "      <td>La Verne</td>\n",
       "      <td>CA</td>\n",
       "      <td>91750.0</td>\n",
       "      <td>5912</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user  card  year  month  day   time   amount           use_chip  \\\n",
       "0     0     0  2002      9    1  06:21  $134.09  Swipe Transaction   \n",
       "1     0     0  2002      9    1  06:42   $38.48  Swipe Transaction   \n",
       "2     0     0  2002      9    2  06:22  $120.34  Swipe Transaction   \n",
       "3     0     0  2002      9    2  17:45  $128.95  Swipe Transaction   \n",
       "4     0     0  2002      9    3  06:23  $104.71  Swipe Transaction   \n",
       "\n",
       "   merchant_id  merchant_city merchant_state      zip   mcc errors is_fraud  \\\n",
       "0        64518       La Verne             CA  91750.0  5300   <NA>       No   \n",
       "1        43058  Monterey Park             CA  91754.0  5411   <NA>       No   \n",
       "2        43058  Monterey Park             CA  91754.0  5411   <NA>       No   \n",
       "3        63932  Monterey Park             CA  91754.0  5651   <NA>       No   \n",
       "4        76114       La Verne             CA  91750.0  5912   <NA>       No   \n",
       "\n",
       "   card_id  is_train  \n",
       "0        0      True  \n",
       "1        0      True  \n",
       "2        0      True  \n",
       "3        0      True  \n",
       "4        0      True  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop user, card, columns\n",
    "gdf = gdf.drop(columns=['user', 'card'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start our data cleaning with slicing off the dollar symbol prefix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['amount'] = gdf['amount'].str.slice(1).astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then convert categorical features like `merchant_city`, `merchant_state`, `zip` and `mcc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['merchant_city'] = gdf['merchant_city'].astype('category')\n",
    "gdf['merchant_state'] = gdf['merchant_state'].astype('category')\n",
    "gdf['zip'] = gdf['zip'].astype('category')\n",
    "gdf['mcc'] = gdf['mcc'].astype('category')                                            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with features with Multiple Categorical values for a transaction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We deal with errors column which can have multiple errors for a transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_errors = ['Bad CVV',\n",
    " 'Bad Zipcode',\n",
    " 'Bad PIN',\n",
    " 'Technical Glitch',\n",
    " 'Insufficient Balance',\n",
    " 'Bad Expiration',\n",
    " 'Bad Card Number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 7 possible errors that a transaction can be associated with in this dataset. These are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bad CVV', 'Bad Zipcode', 'Bad PIN', 'Technical Glitch', 'Insufficient Balance', 'Bad Expiration', 'Bad Card Number']\n"
     ]
    }
   ],
   "source": [
    "print(unique_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For e.g. a given transaction can have multiple errors like`Bad CVV`, `Bad Zipcode` associated with it. We can perform multi-hot encoding which will ensure that for every error that appears we encode it with `1` and `0` otherwise. So across our 7 unique errors if a transaction has `Bad CVV`, `Bad Zipcode` erros then it will be encoded as `[1, 1, 0, 0, 0, 0, 0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploded = gdf['errors'].str.strip(',').str.split(',').explode()\n",
    "\n",
    "raw_one_hot = cudf.get_dummies(exploded, columns=[\"errors\"])\n",
    "errs = raw_one_hot.groupby(raw_one_hot.index).sum()\n",
    "\n",
    "gdf = cudf.concat([gdf, errs], axis=1)\n",
    "\n",
    "gdf = gdf.rename(columns={col:f'errors_{col}' for col in unique_errors})\n",
    "gdf = gdf.drop(columns=['errors'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we extract out the `hour` and `minute` from `time` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf['hour'] = gdf['time'].str.slice(stop=2).astype('int32')\n",
    "gdf['minute'] = gdf['time'].str.slice(start=3).astype('int32')\n",
    "gdf = gdf.drop(columns=['time'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the `X_train`, `X_test` and the labels `y_train` and `y_test` based on the temporal train-test split we did in Notebook 1a where said all transactions in years 1991-2017 would be training set and 2018-2020 would be test set. This corresponds to roughly 85% train and 15% test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gdf = gdf.loc[gdf['is_train'], :]\n",
    "train_gdf = train_gdf.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also make sure that the class distribution of our label remains the same across both train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gdf = gdf.loc[~gdf['is_train'], :]\n",
    "test_gdf = test_gdf.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     99.877801\n",
       "Yes     0.122199\n",
       "Name: is_fraud, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_gdf['is_fraud'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     99.875737\n",
       "Yes     0.124263\n",
       "Name: is_fraud, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gdf['is_fraud'].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_gdf[train_gdf.columns.difference(['is_fraud', 'is_train'])]\n",
    "y_train = train_gdf['is_fraud']\n",
    "X_test = test_gdf[test_gdf.columns.difference(['is_fraud', 'is_train'])]\n",
    "y_test = test_gdf['is_fraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encode the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we label encode our labels using cuML's LabelEncoder (just like in Scikit-learn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have high cardinality categorical columns in our data like `merchant_city`, `merchant_state`, `zip`, `mcc` (stands for merchant category code) with large number of unique categories. If we were to one-hot encode these, our feature set would blow up and we would be hit hard with the curse of dimensionality. Additionally it would lead to either huge memory consumption and very sparse data.  \n",
    "\n",
    "For Categorical Columns with lots of levels instead of One-Hot Encoding we can use [TargetEncoding](https://docs.rapids.ai/api/cuml/stable/api.html?highlight=target%20encoder#cuml.preprocessing.TargetEncoder.TargetEncoder) where each category in the column is replaced with the mean target value for that category. This way we can still effectively represent a categorical column and it only takes up the space of one feature. cuML's implementation of TargetEncoding uses several optimizations to prevent label leakage and parallelize the execution. To learn more about Target Encoding in cuML check out this [target encoder walkthrough](https://medium.com/rapids-ai/target-encoding-with-rapids-cuml-do-more-with-your-categorical-data-8c762c79e784)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuml.preprocessing import TargetEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_card_cols = [\"merchant_city\", \"merchant_state\", \"zip\", \"mcc\"]\n",
    "for col in high_card_cols:\n",
    "    # we append TE to column name to indicate we have target encoded it\n",
    "    out_col = f'{col}_TE'\n",
    "    tgt_encoder = TargetEncoder(smooth=0.001)\n",
    "    X_train[out_col] = tgt_encoder.fit_transform(X_train[col], y_train)\n",
    "    X_test[out_col] = tgt_encoder.transform(X_test[col])\n",
    "# drop old columns\n",
    "X_train.drop(columns=high_card_cols, inplace=True)\n",
    "X_test.drop(columns=high_card_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " As discussed above, we will now one-hot encode rest of the low cardinality categorical columns like `use_chip` which has 3 unique categories. We can easily accomplish this through cudf's get_dummies function (just like in Pandas)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "oneh_enc_cols = [\"use_chip\"]\n",
    "X_train = cudf.get_dummies(X_train)\n",
    "X_test = cudf.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we saved the processed train and test dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['label'] = y_train\n",
    "X_test['label'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_parquet(os.path.join(processed_path, 'X_train.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.to_parquet(os.path.join(processed_path, 'X_test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
