{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mortgage Workflow\n",
    "\n",
    "The original implementation can be found at the [rapidsai notebooks-extendeds](https://github.com/rapidsai/notebooks-extended/blob/462b3b9/intermediate_notebooks/E2E/mortgage/mortgage_e2e.ipynb) site. This notebook is a re-implementation using `gQuant`.\n",
    "\n",
    "## The Dataset\n",
    "The dataset used with this workflow is derived from [Fannie Maeâ€™s Single-Family Loan Performance Data](http://www.fanniemae.com/portal/funding-the-market/data/loan-performance-data.html) with all rights reserved by Fannie Mae. This processed dataset is redistributed with permission and consent from Fannie Mae.\n",
    "\n",
    "To acquire this dataset, please visit [RAPIDS Datasets Homepage](https://docs.rapids.ai/datasets/mortgage-data)\n",
    "\n",
    "## Introduction\n",
    "The Mortgage workflow is composed of three core phases:\n",
    "\n",
    "1. ETL - Extract, Transform, Load\n",
    "2. Data Conversion\n",
    "3. ML - Training\n",
    "\n",
    "### ETL\n",
    "Data is \n",
    "1. Read in from storage\n",
    "2. Transformed to emphasize key features\n",
    "3. Loaded into volatile memory for conversion\n",
    "\n",
    "### Data Conversion\n",
    "Features are\n",
    "1. Broken into (labels, data) pairs\n",
    "2. Distributed across dask workers if using Dask.\n",
    "3. Converted into compressed sparse row (CSR) matrix (DMatrix) format for XGBoost\n",
    "\n",
    "### Machine Learning\n",
    "The CSR data is fed into XGBoost or with a distributed training session with Dask-XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARMUP CUDF ENGINE - OPTIONAL\n",
    "import gc  # python garbage collector\n",
    "import cudf\n",
    "\n",
    "# warmup\n",
    "s = cudf.Series([1,2,3,None,4])\n",
    "\n",
    "del(s)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortgage gQuant Workflow for ETL\n",
    "\n",
    "Two modules are provided with this notebook and should be in the same location (directory) as this notebook: `mortgage_common.py` and `mortgage_gquant_plugins.py`. The plugins module contains the individuals tasks for loading the mortgage data from csv (command separated) files into `cudf` dataframes and processing/transforming these dataframes for mortgage delinquency analysis. As an example the gQuant task to calculate loan delinquecy status period features is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('../..')\n",
    "\n",
    "import inspect\n",
    "from mortgage_gquant_plugins import CreateEverFeatures\n",
    "\n",
    "print(inspect.getsource(CreateEverFeatures))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a worfklow by defining tasks and specifying their configuration (parameters) and inputs (for basics tutorial on gQuant refer to [01_tutorial.ipynb](https://github.com/rapidsai/gQuant/blob/master/notebooks/01_tutorial.ipynb) and custom plugins [05_customize_nodes.ipynb](https://github.com/rapidsai/gQuant/blob/master/notebooks/05_customize_nodes.ipynb)). The workflow to calculate the mortgage features and delinquecy is defined in the `mortgage_etl_workflow_def` function in `mortgage_common` module. Its code is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from mortgage_common import mortgage_etl_workflow_def\n",
    "\n",
    "print(inspect.getsource(mortgage_etl_workflow_def))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the mortgage ETL workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gquant.dataframe_flow import TaskGraph\n",
    "\n",
    "task_spec_list = mortgage_etl_workflow_def()\n",
    "task_graph = TaskGraph(task_spec_list)\n",
    "task_graph.draw(format='ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the workflow and inspect the resultant `final_per_acq_df` dataframe. Adjust your paths below to wherever you downloaded the mortgage dataset to. The `mortgage_data` below is assumed to be in the same directory as this notebook (could be a symlink to wherever the actual data resides), otherwise adjust the paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from gquant.dataframe_flow import TaskGraph\n",
    "from mortgage_common import (\n",
    "    mortgage_etl_workflow_def, MortgageTaskNames)\n",
    "\n",
    "# mortgage_data_path = '/datasets/rapids_data/mortgage'\n",
    "mortgage_data_path = './mortgage_data'\n",
    "# _basedir = os.path.abspath('')  # path of current notebook\n",
    "# mortgage_data_path = os.path.join(_basedir, 'mortgage_data')\n",
    "csvfile_names = os.path.join(mortgage_data_path, 'names.csv')\n",
    "acq_data_path = os.path.join(mortgage_data_path, 'acq')\n",
    "perf_data_path = os.path.join(mortgage_data_path, 'perf')\n",
    "\n",
    "# Some files out of the mortgage dataset.\n",
    "csvfile_acqdata = os.path.join(acq_data_path, 'Acquisition_2000Q1.txt')\n",
    "csvfile_perfdata = os.path.join(perf_data_path, 'Performance_2000Q1.txt_0')\n",
    "\n",
    "gquant_task_spec_list = mortgage_etl_workflow_def(\n",
    "    csvfile_names, csvfile_acqdata, csvfile_perfdata)\n",
    "out_list = [MortgageTaskNames.final_perf_acq_task_name]\n",
    "\n",
    "task_graph = TaskGraph(gquant_task_spec_list)\n",
    "\n",
    "(final_perf_acq_df,) = task_graph.run(out_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mortgage Workflow Ouput CUDF Dataframe:\\n', final_perf_acq_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can indirectly see how much memory is being occupied on the GPU by this cudf dataframe. In my case I see `1863 MB` (assuming only this notebook is running and using the GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clear this GPU memory by deleting the cudf dataframe and running python garbage collector to force garbage collection. After clearing I see `207 MB` occupied on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc  # python garbage collector\n",
    "\n",
    "del(final_perf_acq_df)\n",
    "gc.collect()\n",
    "\n",
    "!nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mortgage Workflow Runner\n",
    "\n",
    "The example above loads just one performance and acqusition datafile. The mortgage dataset is broken down into many csv files. The complete worfklow reads these csv files into cudf dataframes, does all the processing on the GPU, then converts to PyArrow table (essentially arrow dataframes), concatenates the arrow tables, and as a final stage converts into one massive pandas dataframe. From this concatenated dataframe the delinquency column is used as labels and the remaining columns are used as training features. The xgboost DMatrix is instantiated from the features and labels and passed to the xgboost booster trainer. The xgboost trainer copies the data to GPU again and trains on GPU.\n",
    "\n",
    "Below we define the complete data training workflow. This is the non-distributed implementation. The dask distributed implementation will follow. The parameters for the mortgage runner are displayed (limited to 2 files). Below I load 12 files for the actual run. Adjust the `part_count` to something manageable on your system. The limitation will be the host RAM for how many dataframes can be concatenated and the DMatrix instantiated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "from gquant.dataframe_flow import (TaskSpecSchema, TaskGraph)\n",
    "\n",
    "from mortgage_common import (\n",
    "    mortgage_etl_workflow_def, generate_mortgage_gquant_run_params_list,\n",
    "    MortgageTaskNames)\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2001  # end_year is inclusive\n",
    "# end_year = 2016  # end_year is inclusive\n",
    "part_count = 2  # the number of data files to train against\n",
    "\n",
    "# ADJUST YOUR MORTGAGE DATAPATH IF DIFFERENT\n",
    "mortgage_data_path = './mortgage_data'\n",
    "\n",
    "gquant_task_spec_list = mortgage_etl_workflow_def()\n",
    "mortgage_run_params_dict_list = generate_mortgage_gquant_run_params_list(\n",
    "    mortgage_data_path, start_year, end_year, part_count, gquant_task_spec_list)\n",
    "\n",
    "mortgage_run_params_dict_list_for_printing = mortgage_run_params_dict_list.copy()\n",
    "for iparams_dict in mortgage_run_params_dict_list_for_printing:\n",
    "    iparams_dict['task_spec_list'] = 'This is gquant_task_spec_list'\n",
    "\n",
    "print('Parameters configuration for Mortgage Workflow Runner '\n",
    "      '(shortened to 2 files)')\n",
    "print(json.dumps(mortgage_run_params_dict_list_for_printing, indent=2))\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #\n",
    "# ADJUST PART_COUNT FOR YOUR SYSTEM MEMORY\n",
    "part_count = 12  # the number of data files to train against\n",
    "# part_count = 4  # the number of data files to train against\n",
    "\n",
    "mortgage_run_params_dict_list = generate_mortgage_gquant_run_params_list(\n",
    "    mortgage_data_path, start_year, end_year, part_count, gquant_task_spec_list)\n",
    "\n",
    "_basedir = os.path.abspath('')  # path of current notebook\n",
    "mortgage_lib_module = os.path.join(_basedir, 'mortgage_gquant_plugins.py')\n",
    "\n",
    "mortgage_workflow_runner_task = {\n",
    "    TaskSpecSchema.task_id:\n",
    "        MortgageTaskNames.mortgage_workflow_runner_task_name,\n",
    "    TaskSpecSchema.node_type: 'MortgageWorkflowRunner',\n",
    "    TaskSpecSchema.conf: {\n",
    "        'mortgage_run_params_dict_list': mortgage_run_params_dict_list\n",
    "    },\n",
    "    TaskSpecSchema.inputs: [],\n",
    "    TaskSpecSchema.filepath: mortgage_lib_module\n",
    "}\n",
    "\n",
    "# Can be multi-gpu. Set ngpus > 1. This is different than dask xgboost\n",
    "# which is distributed multi-gpu i.e. dask-xgboost could distribute on one\n",
    "# node or multiple nodes. In distributed mode the dmatrix is distributed.\n",
    "ngpus = 1\n",
    "xgb_gpu_params = {\n",
    "    'nround':            100,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2 ** 8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'n_gpus':            ngpus,\n",
    "    # 'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    # 'objective':         'gpu:reg:linear',\n",
    "    'objective':         'reg:squarederror',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "xgb_trainer_task = {\n",
    "    TaskSpecSchema.task_id: MortgageTaskNames.xgb_trainer_task_name,\n",
    "    TaskSpecSchema.node_type: 'XgbMortgageTrainer',\n",
    "    TaskSpecSchema.conf: {\n",
    "        'delete_dataframes': False,\n",
    "        'xgb_gpu_params': xgb_gpu_params\n",
    "    },\n",
    "    TaskSpecSchema.inputs: [\n",
    "        MortgageTaskNames.mortgage_workflow_runner_task_name\n",
    "    ],\n",
    "    TaskSpecSchema.filepath: mortgage_lib_module\n",
    "}\n",
    "\n",
    "task_spec_list = [mortgage_workflow_runner_task, xgb_trainer_task]\n",
    "task_graph = TaskGraph(task_spec_list)\n",
    "task_graph.draw(format='ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to `MortgageWorkflowRunner` and `XgbMortgageTrainer` in the `mortgage_gquant_plugins.py` module for the details of the mortage workflow runner and xgboost trainer tasks.\n",
    "\n",
    "Note the novel manner in which gQuant is used. The `mortgage_workflow_runner` actually runs another gQuant workflow defined by `mortgage_etl_workflow_def()` for each set of acquisition and performance csv files. The output from `mortgage_workflow_runner` is a pandas dataframe (concatenated from processing multiple `final_per_acq_df` dataframes). The `xgb_trainer` is used in an atypical manner. It does not output a dataframe, instead it produces an XGBoost booster. Even though we mostly focus on dataframe flow with gQuant, if the tasks input/output something beside dataframes then gQuant will still run the workflow. When a task does not output a dataframe then gQuant does not perform columns validation. Currently, the responsibility is on the end-user to validate or make sure the input/output types match for the wired non-dataframe tasks. Above only the `xgb_trainer` task's output is not a datframe.\n",
    "\n",
    "We can run the workflow now and obtain the XGBoost trained booster. You can monitor the GPU utilization in a terminal using `nvidia-smi`. On my node I have 125GB of host RAM and two 16GB GPU cards. I am able to process 12 dataframes. If I load more than 12 dataframes, then the workflow crashes during DMatrix creation due to out of memory error. The DMatrix instantiation seems to inflate the data temporarily and I run out of memory on the host. In a terminal you can watch with nvidia-smi the utilization on the GPU. During XGBoost training I typically observe:\n",
    "\n",
    "```\n",
    "$ nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
    "pid, process_name, used_gpu_memory [MiB]\n",
    "27774, /home/avolkov/progs/python_installs/miniconda3/envs/py36-rapids/bin/python, 11025 MiB\n",
    "\n",
    "$ watch -n 0.5 nvidia-smi pmon -c 1\n",
    "# gpu        pid  type    sm   mem   enc   dec   command\n",
    "# Idx          #   C/G     %     %     %     %   name\n",
    "    0      27774     C    99    28     0     0   python\n",
    "    1          -     -     -     -     -     -   -\n",
    "\n",
    "```\n",
    "\n",
    "The DMatrix occupies 11GB of GPU memory and XGBoost training is utilizing 99% of compute processing power on the GPU (specifying `ngpus=2` will split the training across two GPUs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = [\n",
    "    MortgageTaskNames.mortgage_workflow_runner_task_name,\n",
    "    MortgageTaskNames.xgb_trainer_task_name\n",
    "]\n",
    "((mortgage_feat_df_pandas, delinq_df_pandas), bst,) = \\\n",
    "    task_graph.run(out_list)\n",
    "# print(mortgage_feat_df_pandas.head())\n",
    "# print(delinq_df_pandas.head())\n",
    "\n",
    "print('XGBOOST BOOSTER:\\n', bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying ngpus=2 will split the training across two GPUs, but in a non-distributed manner (this approach is being [deprecated](https://xgboost.readthedocs.io/en/latest/gpu/#single-node-multi-gpu) in favor of distributed). If you would like to run with 2 GPUs in this manner, convert the cell below to code (from raw format to code select cell and press Esc+y), and run below on 2 GPUs. If you don't have at least two GPUs do not run the cell below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# CLEAN MEMORY FROM RUN BEFORE\n",
    "import gc\n",
    "from contextlib import suppress\n",
    "\n",
    "with suppress(Exception):\n",
    "    del(bst)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "ngpus = 2\n",
    "xgb_gpu_params = {\n",
    "    'nround':            100,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2 ** 8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'n_gpus':            ngpus,\n",
    "    # 'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    # 'objective':         'gpu:reg:linear',\n",
    "    'objective':         'reg:squarederror',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "# By loading existing dataframes no need to re-run\n",
    "# mortgage_workflow_runner task.\n",
    "replace_spec = {\n",
    "    MortgageTaskNames.mortgage_workflow_runner_task_name: {\n",
    "        'load': [mortgage_feat_df_pandas, delinq_df_pandas]\n",
    "    },\n",
    "    MortgageTaskNames.xgb_trainer_task_name: {\n",
    "        TaskSpecSchema.conf: {\n",
    "            'delete_dataframes': False,\n",
    "            'xgb_gpu_params': xgb_gpu_params\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "out_list = [MortgageTaskNames.xgb_trainer_task_name]\n",
    "(bst,) = task_graph.run(out_list, replace=replace_spec)\n",
    "\n",
    "print('XGBOOST BOOSTER:\\n', bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During XGBoost training on 2 GPUs above, I typically observe (pid differs from run to run):\n",
    "```\n",
    "$ nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
    "pid, process_name, used_gpu_memory [MiB]\n",
    "12819, /home/avolkov/progs/python_installs/miniconda3/envs/py36-rapids/bin/python, 5965 MiB\n",
    "12819, /home/avolkov/progs/python_installs/miniconda3/envs/py36-rapids/bin/python, 5905 MiB\n",
    "\n",
    "\n",
    "$ watch -n 0.5 nvidia-smi pmon -c 1\n",
    "# gpu        pid  type    sm   mem   enc   dec   command\n",
    "# Idx          #   C/G     %     %     %     %   name\n",
    "    0      12819     C    98    13     0     0   python\n",
    "    1      12819     C    99    13     0     0   python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DISPLAY CURRENT MEMORY ON HOST AND GPU\n",
    "!free -m\n",
    "!echo\n",
    "!nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DASK Distributed Mortgage Workflow Runner\n",
    "\n",
    "Typically dask with cudf (dask-cudf) or dask with pandas dataframes is used to distribute the dataframe itself. Then operations are performed on the distributed dataframe. The implementation below differs. We will startup GPU dask workers (in my case I have 2 GPUs on the machine) and each worker will run the mortgage workflow to generate a DMatrix. Thus in the end what we will have can be thought of as a distributed DMatrix. It is just two DMatrices one on each worker. This distributed dmatrix is then passed to the dask-xgboost trainer.\n",
    "\n",
    "#### RECOMMEND TO RESTART THE JUPYTER KERNEL\n",
    "\n",
    "To release GPU resources from previous non-distributed runs above I recommend you RESTART the Jupyter kernel and continue with the cells below. Otherwise you might run out of memory and/or you might see additional processes consuming GPU.\n",
    "\n",
    "We start by clearing previous non-distributed run (in case the cells before this one were executed and Jupyter kernel was not restarted), and starting a dask cluster and client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable NCCL P2P. Only necessary for versions of NCCL < 2.4\n",
    "# https://rapidsai.github.io/projects/cudf/en/0.8.0/dask-xgb-10min.html#Disable-NCCL-P2P.-Only-necessary-for-versions-of-NCCL-%3C-2.4\n",
    "%env NCCL_P2P_DISABLE=1\n",
    "\n",
    "# CLEAN MEMORY FROM RUN BEFORE\n",
    "import gc\n",
    "\n",
    "from contextlib import suppress\n",
    "\n",
    "with suppress(Exception):\n",
    "    del(mortgage_feat_df_pandas)\n",
    "\n",
    "with suppress(Exception):\n",
    "    del(delinq_df_pandas)\n",
    "\n",
    "with suppress(Exception):\n",
    "    del(bst)\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "print('\\nHOST RAM')\n",
    "!free -m\n",
    "\n",
    "print('\\nGPU STATUS')\n",
    "# !nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
    "nvsmiquery = !nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
    "# Output will be empty if nothing is happening on GPUs.\n",
    "if len(nvsmiquery) == 1:\n",
    "    print('\\n'.join(nvsmiquery+['No running processes found']))\n",
    "else:\n",
    "    print('\\n'.join(nvsmiquery))\n",
    "\n",
    "print('\\n\\n')\n",
    "# Start cluster and dask client.\n",
    "\n",
    "memory_limit = 128e9\n",
    "threads_per_worker = 4\n",
    "cluster = LocalCUDACluster(\n",
    "    memory_limit=memory_limit,\n",
    "    threads_per_worker=threads_per_worker)\n",
    "client = Client(cluster)\n",
    "\n",
    "print('DASK LOCAL CUDA CLUSTER')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we define the gQuant workflow similar to how we did it before in the non-distributed case except we will use tasks `DaskMortgageWorkflowRunner` and `DaskXgbMortgageTrainer` in the `mortgage_gquant_plugins.py` module. Refer to these tasks in the `mortgage_gquant_plugins.py` for code details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from gquant.dataframe_flow import (TaskSpecSchema, TaskGraph)\n",
    "\n",
    "from mortgage_common import (\n",
    "    mortgage_etl_workflow_def, generate_mortgage_gquant_run_params_list,\n",
    "    MortgageTaskNames)\n",
    "\n",
    "\n",
    "# mortgage_data_path = '/datasets/rapids_data/mortgage'\n",
    "mortgage_data_path = './mortgage_data'\n",
    "\n",
    "# Using some default csv files for testing.\n",
    "gquant_task_spec_list = mortgage_etl_workflow_def()\n",
    "\n",
    "start_year = 2000\n",
    "end_year = 2001  # end_year is inclusive\n",
    "# end_year = 2016  # end_year is inclusive\n",
    "\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! #\n",
    "# ADJUST PART_COUNT FOR YOUR SYSTEM MEMORY\n",
    "# able to do 18 with create_dmatrix_serially set to True\n",
    "# Otherwise need more host RAM. DGX-1 for Analytics has 1TB RAM.\n",
    "# part_count = 16  # the number of data files to train against\n",
    "# create_dmatrix_serially = False\n",
    "part_count = 18  # the number of data files to train against\n",
    "create_dmatrix_serially = True\n",
    "\n",
    "# Use RAPIDS Memory Manager. Seems to work fine without it.\n",
    "use_rmm = False\n",
    "\n",
    "# Clean up intermediate dataframes in the xgboost training task.\n",
    "delete_dataframes = True\n",
    "\n",
    "mortgage_run_params_dict_list = generate_mortgage_gquant_run_params_list(\n",
    "    mortgage_data_path, start_year, end_year, part_count, gquant_task_spec_list)\n",
    "\n",
    "_basedir = os.path.abspath('')  # path of current notebook\n",
    "mortgage_lib_module = os.path.join(_basedir, 'mortgage_gquant_plugins.py')\n",
    "\n",
    "# filter_dask_logger is primarily for displaying the log in the jupyter\n",
    "# notebook. The dask distributed logger is used by gQuant mortgage tasks\n",
    "# when running on dask workers.\n",
    "filter_dask_logger = True\n",
    "\n",
    "mortgage_workflow_runner_task = {\n",
    "    TaskSpecSchema.task_id:\n",
    "        MortgageTaskNames.dask_mortgage_workflow_runner_task_name,\n",
    "    TaskSpecSchema.node_type: 'DaskMortgageWorkflowRunner',\n",
    "    TaskSpecSchema.conf: {\n",
    "        'mortgage_run_params_dict_list': mortgage_run_params_dict_list,\n",
    "        'client': client,\n",
    "        'use_rmm': use_rmm,\n",
    "        'filter_dask_logger': filter_dask_logger\n",
    "    },\n",
    "    TaskSpecSchema.inputs: [],\n",
    "    TaskSpecSchema.filepath: mortgage_lib_module\n",
    "}\n",
    "\n",
    "# task_spec_list = [mortgage_workflow_runner_task]\n",
    "#\n",
    "# out_list = [MortgageTaskNames.dask_mortgage_workflow_runner_task_name]\n",
    "# task_graph = TaskGraph(task_spec_list)\n",
    "# ((mortgage_feat_df_delinq_df_pandas_futures),) = \\\n",
    "#     task_graph.run(out_list)\n",
    "#\n",
    "# print('MORTGAGE_FEAT_DF_DELINQ_DF_PANDAS_FUTURES: ',\n",
    "#       mortgage_feat_df_delinq_df_pandas_futures)\n",
    "\n",
    "dxgb_gpu_params = {\n",
    "    'nround':            100,\n",
    "    'max_depth':         8,\n",
    "    'max_leaves':        2 ** 8,\n",
    "    'alpha':             0.9,\n",
    "    'eta':               0.1,\n",
    "    'gamma':             0.1,\n",
    "    'learning_rate':     0.1,\n",
    "    'subsample':         1,\n",
    "    'reg_lambda':        1,\n",
    "    'scale_pos_weight':  2,\n",
    "    'min_child_weight':  30,\n",
    "    'tree_method':       'gpu_hist',\n",
    "    'n_gpus':            1,\n",
    "    'distributed_dask':  True,\n",
    "    'loss':              'ls',\n",
    "    # 'objective':         'gpu:reg:linear',\n",
    "    'objective':         'reg:squarederror',\n",
    "    'max_features':      'auto',\n",
    "    'criterion':         'friedman_mse',\n",
    "    'grow_policy':       'lossguide',\n",
    "    'verbose':           True\n",
    "}\n",
    "\n",
    "dxgb_trainer_task = {\n",
    "    TaskSpecSchema.task_id: MortgageTaskNames.dask_xgb_trainer_task_name,\n",
    "    TaskSpecSchema.node_type: 'DaskXgbMortgageTrainer',\n",
    "    TaskSpecSchema.conf: {\n",
    "        'create_dmatrix_serially': create_dmatrix_serially,\n",
    "        # Able to load 18 files with create_dmatrix_serially set\n",
    "        # to True. 16 is the max I could do otherwise.\n",
    "        'delete_dataframes': delete_dataframes,\n",
    "        'dxgb_gpu_params': dxgb_gpu_params,\n",
    "        'client': client,\n",
    "        'filter_dask_logger': filter_dask_logger\n",
    "    },\n",
    "    TaskSpecSchema.inputs: [\n",
    "        MortgageTaskNames.dask_mortgage_workflow_runner_task_name\n",
    "    ],\n",
    "    TaskSpecSchema.filepath: mortgage_lib_module\n",
    "}\n",
    "\n",
    "task_spec_list = [mortgage_workflow_runner_task, dxgb_trainer_task]\n",
    "\n",
    "task_graph = TaskGraph(task_spec_list)\n",
    "task_graph.draw(format='ipynb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final step we run the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look in the terminal where the jupyter was launched from\n",
    "# for real-time logging. Otherwise, the logging for individual\n",
    "# tasks is captured and displayed after the workers\n",
    "# complete that task.\n",
    "out_list = [MortgageTaskNames.dask_xgb_trainer_task_name]\n",
    "(bst,) = task_graph.run(out_list)\n",
    "\n",
    "print('XGBOOST BOOSTER:\\n', bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "During Dask-XGBoost training on 2 GPUs above I observed:\n",
    "\n",
    "```\n",
    "$ nvidia-smi --query-compute-apps=pid,process_name,used_memory --format=csv\n",
    "pid, process_name, used_gpu_memory [MiB]\n",
    "15945, /home/avolkov/progs/python_installs/miniconda3/envs/py36-rapids/bin/python, 9135 MiB\n",
    "15946, /home/avolkov/progs/python_installs/miniconda3/envs/py36-rapids/bin/python, 8681 MiB\n",
    "\n",
    "$ watch -n 0.5 nvidia-smi pmon -c 1\n",
    "# gpu        pid  type    sm   mem   enc   dec   command\n",
    "# Idx          #   C/G     %     %     %     %   name\n",
    "    0      15945     C    99    13     0     0   python\n",
    "    1      15946     C    99    12     0     0   python\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "We re-implemented the RAPIDS mortgage ETL notebook using gQuant. The benefit of the gQuant implementation is that one can readily break down their workflow into modular parts. It becomes much easier to understand and optimize the individual components of the workflow pipeline.\n",
    "\n",
    "A non-distributed and a distributed version was demonstrated. The benefits of distributed dask version were that more data was processed (18 files vs 12 files which amounts to 6GB of more data) and the dask-distributed version ran ETL in parallel on two workers (one worker per GPU) thus speeding up ETL. Using distributed version we could scale to multiple nodes as well.\n",
    "\n",
    "Two scripts are provided along with this notebook `mortgage_run_workflow_local.py` and `mortgage_run_workflow_daskdistrib.py` which run similar code to what was presented in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLEAN UP\n",
    "client.close()\n",
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
