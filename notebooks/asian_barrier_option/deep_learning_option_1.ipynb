{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Barrier Option\n",
    "\n",
    "We used Numba and CuPy in the previous notebook to run Monte Carlo simulation to determine the price of the barrier option. A Monte Carlo simulation needs millions of paths to get an accurate answer which is computationally intensive. Another approach is to train a neural network to learn the pricing model. In the following notebook, we will use Monte Carlo to generate a dataset and train a Fully connected network to learn the pricing model.\n",
    "\n",
    "### Batched Data generation\n",
    "\n",
    "The dataset is an important part of the Deep learning training. We will modify the previous single Asian Barrier Option pricing code to handle a batch of Barrier Option pricing. Loading all the necessary libraries:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "from torch.utils.dlpack import from_dlpack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CuPy version of batched barrier option pricing simulation is as follows:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
    "extern \"C\" __global__ void batched_barrier_option(\n",
    "    float *d_s,\n",
    "    const float T,\n",
    "    const float * K,\n",
    "    const float * B,\n",
    "    const float * S0,\n",
    "    const float * sigma,\n",
    "    const float * mu,\n",
    "    const float * r,\n",
    "    const float * d_normals,\n",
    "    const long N_STEPS,\n",
    "    const long N_PATHS,\n",
    "    const long N_BATCH)\n",
    "{\n",
    "  unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  unsigned stride = blockDim.x * gridDim.x;\n",
    "  unsigned tid = threadIdx.x;\n",
    "  const float tmp3 = sqrt(T/N_STEPS);\n",
    "\n",
    "\n",
    "  for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
    "  {\n",
    "    int batch_id = i / N_PATHS;\n",
    "    int path_id = i % N_PATHS;\n",
    "    float s_curr = S0[batch_id];\n",
    "    float tmp1 = mu[batch_id]*T/N_STEPS;\n",
    "    float tmp2 = exp(-r[batch_id]*T);\n",
    "    unsigned n=0;\n",
    "    double running_average = 0.0;\n",
    "    for(unsigned n = 0; n < N_STEPS; n++){\n",
    "       s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
    "       running_average += (s_curr - running_average) / (n + 1.0);\n",
    "       if (running_average <= B[batch_id]){\n",
    "           break;\n",
    "       }\n",
    "    }\n",
    "\n",
    "    float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
    "    d_s[i] = tmp2 * payoff;\n",
    "  }\n",
    "}\n",
    "\n",
    "''', 'batched_barrier_option')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing it out by entering two sets of option parameters:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PATHS = 8192000\n",
    "N_STEPS = 365\n",
    "N_BATCH = 2\n",
    "T = 1.0\n",
    "\n",
    "K = cupy.array([110.0, 120.0], dtype=cupy.float32)\n",
    "B = cupy.array([100.0, 90.0], dtype=cupy.float32)\n",
    "S0 = cupy.array([120.0, 100.0], dtype=cupy.float32)\n",
    "sigma = cupy.array([0.35, 0.2], dtype=cupy.float32)\n",
    "mu = cupy.array([0.15, 0.1], dtype=cupy.float32)\n",
    "r =cupy.array([0.05, 0.05], dtype=cupy.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time 0.31394100189208984 v [21.240282    0.84774816]\n"
     ]
    }
   ],
   "source": [
    "def batch_run():\n",
    "    number_of_threads = 256\n",
    "    number_of_blocks = (N_PATHS * N_BATCH - 1) // number_of_threads + 1\n",
    "    randoms_gpu = cupy.random.normal(0, 1, N_BATCH*N_PATHS * N_STEPS, dtype=cupy.float32)\n",
    "    output = cupy.zeros(N_BATCH*N_PATHS, dtype=cupy.float32)\n",
    "    cupy.cuda.stream.get_current_stream().synchronize()\n",
    "    s = time.time()\n",
    "    cupy_batched_barrier_option((number_of_blocks,), (number_of_threads,),\n",
    "                       (output, np.float32(T), K, B, S0, sigma, mu, r,\n",
    "                        randoms_gpu, N_STEPS, N_PATHS, N_BATCH))\n",
    "    v = output.reshape(N_BATCH, N_PATHS).mean(axis=1)\n",
    "    cupy.cuda.stream.get_current_stream().synchronize()\n",
    "    e = time.time()\n",
    "    print('time', e-s, 'v',v)\n",
    "batch_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works efficiently hence we will construct an `OptionDataSet` class to wrap the above code so we can use it in Pytorch. Note how we implemented the iterable Dataset interface:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptionDataSet(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
    "        self.num = 0\n",
    "        self.max_length = max_len\n",
    "        self.N_PATHS = number_path\n",
    "        self.N_STEPS = 365\n",
    "        self.N_BATCH = batch\n",
    "        self.T = np.float32(1.0)\n",
    "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
    "        self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
    "        self.number_of_threads = threads\n",
    "        cupy.random.seed(seed)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_length\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.num > self.max_length:\n",
    "            raise StopIteration\n",
    "        X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
    "        X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
    "        X[:, 1] = X[:, 0] * X[:, 1]\n",
    "        randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
    "        cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
    "                              cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
    "        Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
    "        self.num += 1\n",
    "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Writing the dataset generated by the CuPy method into a file `cupy_dataset.py`:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting cupy_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile cupy_dataset.py \n",
    "import cupy\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "\n",
    "cupy_batched_barrier_option = cupy.RawKernel(r'''\n",
    "extern \"C\" __global__ void batched_barrier_option(\n",
    "    float *d_s,\n",
    "    const float T,\n",
    "    const float * K,\n",
    "    const float * B,\n",
    "    const float * S0,\n",
    "    const float * sigma,\n",
    "    const float * mu,\n",
    "    const float * r,\n",
    "    const float * d_normals,\n",
    "    const long N_STEPS,\n",
    "    const long N_PATHS,\n",
    "    const long N_BATCH)\n",
    "{\n",
    "  unsigned idx =  threadIdx.x + blockIdx.x * blockDim.x;\n",
    "  unsigned stride = blockDim.x * gridDim.x;\n",
    "  unsigned tid = threadIdx.x;\n",
    "  const float tmp3 = sqrt(T/N_STEPS);\n",
    "\n",
    "\n",
    "  for (unsigned i = idx; i<N_PATHS * N_BATCH; i+=stride)\n",
    "  {\n",
    "    int batch_id = i / N_PATHS;\n",
    "    int path_id = i % N_PATHS;\n",
    "    float s_curr = S0[batch_id];\n",
    "    float tmp1 = mu[batch_id]*T/N_STEPS;\n",
    "    float tmp2 = exp(-r[batch_id]*T);\n",
    "    unsigned n=0;\n",
    "    double running_average = 0.0;\n",
    "    for(unsigned n = 0; n < N_STEPS; n++){\n",
    "       s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH];\n",
    "       running_average += (s_curr - running_average) / (n + 1.0);\n",
    "       if (running_average <= B[batch_id]){\n",
    "           break;\n",
    "       }\n",
    "    }\n",
    "\n",
    "    float payoff = (running_average>K[batch_id] ? running_average-K[batch_id] : 0.f); \n",
    "    d_s[i] = tmp2 * payoff;\n",
    "  }\n",
    "}\n",
    "\n",
    "''', 'batched_barrier_option')\n",
    "\n",
    "class OptionDataSet(torch.utils.data.IterableDataset):\n",
    "    \n",
    "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=256,seed=15):\n",
    "        self.num = 0\n",
    "        self.max_length = max_len\n",
    "        self.N_PATHS = number_path\n",
    "        self.N_STEPS = 365\n",
    "        self.N_BATCH = batch\n",
    "        self.T = np.float32(1.0)\n",
    "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
    "        self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
    "        self.number_of_threads = threads\n",
    "        cupy.random.seed(seed)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_length\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.num > self.max_length:\n",
    "            raise StopIteration\n",
    "        X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
    "        X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
    "        X[:, 1] = X[:, 0] * X[:, 1]\n",
    "        randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
    "        cupy_batched_barrier_option((self.number_of_blocks,), (self.number_of_threads,), (self.output, self.T, cupy.ascontiguousarray(X[:, 0]), \n",
    "                              cupy.ascontiguousarray(X[:, 1]), cupy.ascontiguousarray(X[:, 2]), cupy.ascontiguousarray(X[:, 3]), cupy.ascontiguousarray(X[:, 4]), cupy.ascontiguousarray(X[:, 5]), randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH))\n",
    "        Y = self.output.reshape(self.N_BATCH, self.N_PATHS).mean(axis=1)\n",
    "        self.num += 1\n",
    "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It generates random parameters for the option and computes the price by running Monte Carlo simulation. Here is a test code to sample 10 data points with batch size 16:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6564e+02, 0.0000e+00, 8.0078e+01, 1.0865e+02, 8.5974e-03, 0.0000e+00,\n",
      "        2.7736e+01, 0.0000e+00, 0.0000e+00, 6.4282e+01, 0.0000e+00, 5.1417e+00,\n",
      "        0.0000e+00, 1.4734e+02, 4.1784e+01, 0.0000e+00], device='cuda:0')\n",
      "tensor([ 51.4447,   0.3706, 116.7244,  33.6892,  45.7084,   0.0000,   5.1548,\n",
      "         71.4430,  64.4561,   7.6365,   0.0000,   4.7105,  68.6678,   0.0000,\n",
      "          0.0000,  17.2087], device='cuda:0')\n",
      "tensor([1.3601e+02, 8.4572e+01, 3.2238e+01, 0.0000e+00, 2.3316e+01, 6.3973e+00,\n",
      "        2.2829e+01, 5.3734e+01, 5.1836e+01, 8.7640e+01, 2.8562e-06, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9714e+00], device='cuda:0')\n",
      "tensor([3.7228e+00, 1.0321e+02, 1.0907e-01, 4.6117e+01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.3699e-03, 2.3906e+01, 4.2589e+01, 5.9842e+01, 5.6827e+01,\n",
      "        8.0516e+01, 2.5004e+01, 0.0000e+00, 0.0000e+00], device='cuda:0')\n",
      "tensor([2.6344e+01, 1.8291e+01, 0.0000e+00, 1.2766e+02, 2.1523e+01, 4.5279e+00,\n",
      "        4.7861e-01, 0.0000e+00, 8.2980e-06, 0.0000e+00, 6.8945e-02, 7.0406e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.5406e+02, 1.2176e+01], device='cuda:0')\n",
      "tensor([1.2431e+02, 1.9494e+01, 5.1725e+01, 0.0000e+00, 1.2431e-02, 2.0234e+01,\n",
      "        0.0000e+00, 1.3685e+01, 1.1949e+02, 1.6619e-04, 1.0885e+01, 1.1326e+02,\n",
      "        1.1969e+02, 0.0000e+00, 2.3736e-04, 1.3548e+02], device='cuda:0')\n",
      "tensor([110.9058,   0.0000,   0.0000,   0.0000,  43.5226,  83.1457,   0.0000,\n",
      "         22.4567,  20.7376,   1.0246,   6.9053,   0.0000, 104.7720,   0.0000,\n",
      "         13.4384,   0.0000], device='cuda:0')\n",
      "tensor([0.0000e+00, 8.3981e+00, 1.3400e-03, 1.4575e+02, 0.0000e+00, 1.0917e+02,\n",
      "        0.0000e+00, 3.0750e+01, 8.3270e+00, 4.8134e+01, 1.3687e+02, 0.0000e+00,\n",
      "        4.2993e+01, 3.4607e+01, 0.0000e+00, 0.0000e+00], device='cuda:0')\n",
      "tensor([  0.0000,   0.0000, 117.3532,  99.8260,   3.3395,   0.9343,   7.4268,\n",
      "          0.0000,   0.0000,   0.0000,  74.1327,   1.0125,   0.0000, 131.3156,\n",
      "         31.8618, 172.4400], device='cuda:0')\n",
      "tensor([5.3291e+01, 5.0224e+01, 0.0000e+00, 1.1189e+02, 1.1350e+02, 1.1941e+01,\n",
      "        2.7495e+00, 0.0000e+00, 4.0003e-04, 0.0000e+00, 4.5845e+01, 3.7198e+01,\n",
      "        1.2130e-02, 9.3004e+01, 1.0681e+01, 2.2738e-01], device='cuda:0')\n",
      "tensor([  0.0000,   0.0000,   0.0000,  49.9597,   0.0000, 149.2885,   5.3984,\n",
      "        116.8099,  87.6783,  16.2566,  81.6230,  25.5476,   0.0000,   0.0000,\n",
      "        123.4632,   0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from cupy_dataset import OptionDataSet\n",
    "ds = OptionDataSet(10, number_path=1000000, batch=16, seed=15)\n",
    "for i in ds:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement the same code by Numba:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.6564e+02, 0.0000e+00, 8.0078e+01, 1.0865e+02, 8.5974e-03, 0.0000e+00,\n",
      "        2.7736e+01, 0.0000e+00, 0.0000e+00, 6.4282e+01, 0.0000e+00, 5.1417e+00,\n",
      "        0.0000e+00, 1.4734e+02, 4.1784e+01, 0.0000e+00], device='cuda:0')\n",
      "tensor([ 51.4447,   0.3706, 116.7244,  33.6892,  45.7084,   0.0000,   5.1548,\n",
      "         71.4430,  64.4561,   7.6365,   0.0000,   4.7105,  68.6678,   0.0000,\n",
      "          0.0000,  17.2087], device='cuda:0')\n",
      "tensor([1.3601e+02, 8.4572e+01, 3.2238e+01, 0.0000e+00, 2.3316e+01, 6.3973e+00,\n",
      "        2.2829e+01, 5.3734e+01, 5.1836e+01, 8.7640e+01, 2.8562e-06, 0.0000e+00,\n",
      "        0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9714e+00], device='cuda:0')\n",
      "tensor([3.7228e+00, 1.0321e+02, 1.0907e-01, 4.6117e+01, 0.0000e+00, 0.0000e+00,\n",
      "        0.0000e+00, 9.3699e-03, 2.3906e+01, 4.2589e+01, 5.9842e+01, 5.6827e+01,\n",
      "        8.0516e+01, 2.5004e+01, 0.0000e+00, 0.0000e+00], device='cuda:0')\n",
      "tensor([2.6344e+01, 1.8291e+01, 0.0000e+00, 1.2766e+02, 2.1523e+01, 4.5279e+00,\n",
      "        4.7861e-01, 0.0000e+00, 8.2982e-06, 0.0000e+00, 6.8945e-02, 7.0406e+00,\n",
      "        0.0000e+00, 0.0000e+00, 1.5406e+02, 1.2176e+01], device='cuda:0')\n",
      "tensor([1.2431e+02, 1.9494e+01, 5.1725e+01, 0.0000e+00, 1.2431e-02, 2.0234e+01,\n",
      "        0.0000e+00, 1.3685e+01, 1.1949e+02, 1.6619e-04, 1.0885e+01, 1.1326e+02,\n",
      "        1.1969e+02, 0.0000e+00, 2.3736e-04, 1.3548e+02], device='cuda:0')\n",
      "tensor([110.9058,   0.0000,   0.0000,   0.0000,  43.5226,  83.1457,   0.0000,\n",
      "         22.4567,  20.7376,   1.0246,   6.9053,   0.0000, 104.7720,   0.0000,\n",
      "         13.4384,   0.0000], device='cuda:0')\n",
      "tensor([0.0000e+00, 8.3981e+00, 1.3400e-03, 1.4575e+02, 0.0000e+00, 1.0917e+02,\n",
      "        0.0000e+00, 3.0750e+01, 8.3270e+00, 4.8134e+01, 1.3687e+02, 0.0000e+00,\n",
      "        4.2993e+01, 3.4607e+01, 0.0000e+00, 0.0000e+00], device='cuda:0')\n",
      "tensor([  0.0000,   0.0000, 117.3532,  99.8260,   3.3395,   0.9343,   7.4268,\n",
      "          0.0000,   0.0000,   0.0000,  74.1327,   1.0125,   0.0000, 131.3156,\n",
      "         31.8618, 172.4400], device='cuda:0')\n",
      "tensor([5.3291e+01, 5.0224e+01, 0.0000e+00, 1.1189e+02, 1.1350e+02, 1.1941e+01,\n",
      "        2.7495e+00, 0.0000e+00, 4.0003e-04, 0.0000e+00, 4.5845e+01, 3.7198e+01,\n",
      "        1.2130e-02, 9.3004e+01, 1.0681e+01, 2.2738e-01], device='cuda:0')\n",
      "tensor([  0.0000,   0.0000,   0.0000,  49.9597,   0.0000, 149.2885,   5.3984,\n",
      "        116.8099,  87.6783,  16.2566,  81.6230,  25.5476,   0.0000,   0.0000,\n",
      "        123.4632,   0.0000], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import numba\n",
    "from numba import cuda\n",
    "\n",
    "@cuda.jit\n",
    "def batch_barrier_option(d_s, T, K, B, S0, sigma, mu, r, d_normals, N_STEPS, N_PATHS, N_BATCH):\n",
    "    # ii - overall thread index\n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    stride = cuda.gridDim.x * cuda.blockDim.x\n",
    "    tmp3 = math.sqrt(T/N_STEPS)\n",
    "    for i in range(ii, N_PATHS * N_BATCH, stride):\n",
    "        batch_id = i // N_PATHS\n",
    "        path_id = i % N_PATHS\n",
    "        tmp1 = mu[batch_id]*T/N_STEPS\n",
    "        tmp2 = math.exp(-r[batch_id]*T)\n",
    "        running_average = 0.0\n",
    "        s_curr = S0[batch_id]\n",
    "        for n in range(N_STEPS):\n",
    "\n",
    "            s_curr += tmp1 * s_curr + sigma[batch_id]*s_curr*tmp3*d_normals[path_id + batch_id * N_PATHS + n * N_PATHS * N_BATCH]\n",
    "            running_average = running_average + 1.0/(n + 1.0) * (s_curr - running_average)\n",
    "            if i==0 and batch_id == 2:\n",
    "                print(s_curr)\n",
    "            if running_average <= B[batch_id]:\n",
    "                break\n",
    "        payoff = running_average - K[batch_id] if running_average > K[batch_id] else 0\n",
    "        d_s[i] = tmp2 * payoff\n",
    "\n",
    "class NumbaOptionDataSet(object):\n",
    "    \n",
    "    def __init__(self, max_len=10, number_path = 1000, batch=2, threads=512, seed=15):\n",
    "        self.num = 0\n",
    "        self.max_length = max_len\n",
    "        self.N_PATHS = number_path\n",
    "        self.N_STEPS = 365\n",
    "        self.N_BATCH = batch\n",
    "        self.T = np.float32(1.0)\n",
    "        self.output = cupy.zeros(self.N_BATCH*self.N_PATHS, dtype=cupy.float32) \n",
    "        self.number_of_blocks = (self.N_PATHS * self.N_BATCH - 1) // threads + 1\n",
    "        self.number_of_threads = threads\n",
    "        cupy.random.seed(seed)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.max_length\n",
    "        \n",
    "    def __iter__(self):\n",
    "        self.num = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self.num > self.max_length:\n",
    "            raise StopIteration\n",
    "        X = cupy.random.rand(self.N_BATCH, 6, dtype=cupy.float32)\n",
    "        X = X * cupy.array([200.0, 0.99, 200.0, 0.4, 0.2, 0.2], dtype=cupy.float32)\n",
    "        X[:, 1] = X[:, 0] * X[:, 1]\n",
    "        randoms = cupy.random.normal(0, 1, self.N_BATCH * self.N_PATHS * self.N_STEPS, dtype=cupy.float32)\n",
    "        batch_barrier_option[(self.number_of_blocks,), (self.number_of_threads,)](self.output, self.T, X[:, 0], \n",
    "                              X[:, 1], X[:, 2], X[:, 3], X[:, 4], X[:, 5], randoms, self.N_STEPS, self.N_PATHS, self.N_BATCH)\n",
    "        o = self.output.reshape(self.N_BATCH, self.N_PATHS)\n",
    "        Y = o.mean(axis = 1) \n",
    "        self.num += 1\n",
    "        return (from_dlpack(X.toDlpack()), from_dlpack(Y.toDlpack()))\n",
    "ds = NumbaOptionDataSet(10, number_path=1000000, batch=16, seed=15)\n",
    "for i in ds:\n",
    "    print(i[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "To map the option parameters to price, we use 6 layers of fully connected neural network with hidden dimension 512 as inspired by [this paper](https://arxiv.org/abs/1809.02233). Writing this model into a file `model.py`:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, hidden=512):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(6, hidden)\n",
    "        self.fc2 = nn.Linear(hidden, hidden)\n",
    "        self.fc3 = nn.Linear(hidden, hidden)\n",
    "        self.fc4 = nn.Linear(hidden, hidden)\n",
    "        self.fc5 = nn.Linear(hidden, hidden)\n",
    "        self.fc6 = nn.Linear(hidden, 1)\n",
    "        self.register_buffer('norm',\n",
    "                             torch.tensor([200.0,\n",
    "                                           198.0,\n",
    "                                           200.0,\n",
    "                                           0.4,\n",
    "                                           0.2,\n",
    "                                           0.2]))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x / self.norm\n",
    "        x = F.elu(self.fc1(x))\n",
    "        x = F.elu(self.fc2(x))\n",
    "        x = F.elu(self.fc3(x))\n",
    "        x = F.elu(self.fc4(x))\n",
    "        x = F.elu(self.fc5(x))\n",
    "        return self.fc6(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we know the random parameters' scaling factors, the input parameters are first scaled back to a range of (0-1) by dividing them by (200.0, 198.0, 200.0, 0.4, 0.2, 0.2). Then they are projected 5 times to the hidden dimension of 512 after the `ReLu` activation function. The last layer is a linear layer that maps the hidden dimension to the predicted option price. It is chosen because we need to compute the second order differentiation of the parameters. If use ReLu, the second order differentiation will always be zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we use [Ignite](https://github.com/pytorch/ignite) which is a high-level library to train neural networks in PyTorch. We use `MSELoss` as the loss function, `Adam` as the optimizer and `CosineAnnealingScheduler` as the learning rate scheduler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e070b0de8df9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mlog_interval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCosineAnnealingScheduler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_event_handler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mITERATION_STARTED\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "from ignite.engine import Engine, Events\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from model import Net\n",
    "from cupy_dataset import OptionDataSet\n",
    "\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "\n",
    "def train_update(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred[:,0], y)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "log_interval = 100\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output)\n",
    "        \n",
    "trainer.run(dataset, max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorCore mixed precision training\n",
    "\n",
    "The V100 GPUs have 640 tensor cores that can accelerate half precision multiplication and single precision accumulation. [Apex library](https://github.com/NVIDIA/apex) developed by NVIDIA makes mixed precision and distributed training in Pytorch easy. By changing 3 lines of code, it can use the tensor cores to accelerate the training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected optimization level O1:  Insert automatic casts around Pytorch functions and Tensor methods.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O1\n",
      "cast_model_type        : None\n",
      "patch_torch_functions  : True\n",
      "keep_batchnorm_fp32    : None\n",
      "master_weights         : None\n",
      "loss_scale             : dynamic\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 32768.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 16384.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 8192.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 4096.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 2048.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 1024.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 512.0\n",
      "Gradient overflow.  Skipping step, loss scaler 0 reducing loss scale to 256.0\n",
      "loss 347.87542724609375\n",
      "loss 11.251447677612305\n"
     ]
    }
   ],
   "source": [
    "from apex import amp\n",
    "from ignite.engine import Engine, Events\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from model import Net\n",
    "from cupy_dataset import OptionDataSet\n",
    "\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "opt_level = 'O1'\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
    "dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=12800)\n",
    "\n",
    "def train_update(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred[:,0], y)\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "log_interval = 100\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output)\n",
    "        \n",
    "trainer.run(dataset, max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apex makes multiple GPU training easy. We just make sure the DataSet in different processes generate the data independent of each other and wrap the model into the `DistributedDataParallel` model. However, to launch distributed training, we need to put everything into a python file. Following is an example:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile distributed_train.py \n",
    "import cupy\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.dlpack import from_dlpack\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from apex import amp\n",
    "from ignite.engine import Engine, Events\n",
    "from torch.nn import MSELoss\n",
    "from torch.optim import Adam\n",
    "from ignite.contrib.handlers.param_scheduler import CosineAnnealingScheduler\n",
    "from ignite.handlers import ModelCheckpoint\n",
    "from apex.parallel import DistributedDataParallel \n",
    "import argparse\n",
    "from model import Net\n",
    "from cupy_dataset import OptionDataSet\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--local_rank\", default=0, type=int)\n",
    "args = parser.parse_args()\n",
    "\n",
    "args.distributed = False\n",
    "if 'WORLD_SIZE' in os.environ:\n",
    "    args.distributed = int(os.environ['WORLD_SIZE']) > 1\n",
    "\n",
    "if args.distributed:\n",
    "    torch.cuda.set_device(args.local_rank)\n",
    "    torch.distributed.init_process_group(backend='nccl',\n",
    "                                         init_method='env://')\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "\n",
    "model = Net().cuda()\n",
    "loss_fn = MSELoss()\n",
    "optimizer = Adam(model.parameters(), lr=1e-3)\n",
    "opt_level = 'O1'\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=opt_level)\n",
    "if args.distributed:\n",
    "    model = DistributedDataParallel(model)\n",
    "dataset = OptionDataSet(max_len=10000, number_path = 1024, batch=10240, seed=args.local_rank)\n",
    "\n",
    "def train_update(engine, batch):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred[:,0], y)\n",
    "    with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "        scaled_loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "trainer = Engine(train_update)\n",
    "log_interval = 100\n",
    "\n",
    "scheduler = CosineAnnealingScheduler(optimizer, 'lr', 1e-4, 1e-6, len(dataset))\n",
    "trainer.add_event_handler(Events.ITERATION_STARTED, scheduler)\n",
    "    \n",
    "@trainer.on(Events.ITERATION_COMPLETED)\n",
    "def log_training_loss(engine):\n",
    "    iter = (engine.state.iteration - 1) % len(dataset) + 1\n",
    "    if iter % log_interval == 0:\n",
    "        print('loss', engine.state.output)\n",
    "        \n",
    "trainer.run(dataset, max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To launch multiple processes training, we need to run the following command:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m torch.distributed.launch --nproc_per_node=4 distributed_train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works and all the GPUs are busy to train this network. However, it has a few problems:-\n",
    "   \n",
    "    1. There is no model serialization so the trained model is not saved\n",
    "    2. There is no validation dataset to check the training progress\n",
    "    3. Most of the time is spent in Monte Carlo simulation hence the training is slow\n",
    "    4. We use a few paths(1024) for each option parameter set which is noise and the model cannot converge to a low cost value.\n",
    "We will address these problems in the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
