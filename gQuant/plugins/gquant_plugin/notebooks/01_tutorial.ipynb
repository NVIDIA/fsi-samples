{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to greenflow\n",
    "\n",
    "**greenflow** is a set of open-source examples for Quantitative Analysis tasks:\n",
    "- Data preparation & feat. engineering\n",
    "- Alpha seeking modeling\n",
    "- Technical indicators\n",
    "- Backtesting\n",
    "\n",
    "It is GPU-accelerated by leveraging [**RAPIDS.ai**](https://rapids.ai) technology, and has Multi-GPU and Multi-Node support.\n",
    "\n",
    "greenflow computing components are oriented around its plugins and task graph.\n",
    "\n",
    "## Download example datasets\n",
    "\n",
    "Before getting started, let's download the example datasets if not present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset is already present. No need to re-download it.\n"
     ]
    }
   ],
   "source": [
    "! ((test ! -f './data/stock_price_hist.csv.gz' ||  test ! -f './data/security_master.csv.gz') && \\\n",
    "  cd .. && bash download_data.sh) || echo \"Dataset is already present. No need to re-download it.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About this notebook\n",
    "\n",
    "In this tutorial, we are going to use greenflow to do a simple quant job. The job tasks are listed below:\n",
    "    1. load csv stock data.\n",
    "    2. filter out the stocks that has average volume smaller than 50.\n",
    "    3. sort the stock symbols and datetime.\n",
    "    4. add rate of return as a feature into the table.\n",
    "    5. in two branches, computethe mean volume and mean return.\n",
    "    6. read the file containing the stock symbol names, and join the computed dataframes.\n",
    "    7. output the result in csv files.\n",
    "    \n",
    "## TaskGraph playground\n",
    "\n",
    "Run the following greenflow code to start a empty TaskGraph where computation graph can be created. You can follow the steps as listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ec73b8f328745fd8403d1821acd4ffb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "from greenflow.dataframe_flow import TaskGraph\n",
    "task_graph = TaskGraph()\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by Step to build your first task graph\n",
    "\n",
    "### Create Task node to load the included stock csv file \n",
    "<img src=\"images/loader_csv.gif\" align=\"center\">\n",
    "\n",
    "### Explore the data and visualize it\n",
    "<img src='images/explore_data.gif' align='center'>\n",
    "\n",
    "### Clean up the Task nodes for next steps\n",
    "<img src='images/clean.gif' align='center'>\n",
    "\n",
    "### Filter the data and compute the rate of return feature\n",
    "<img src='images/get_return_feature.gif' align='center'>\n",
    "\n",
    "### Save current TaskGraph for a composite Task node\n",
    "<img src='images/add_composite_node.gif' align='center'>\n",
    "\n",
    "### Clean up the redudant feature computation Task nodes\n",
    "<img src='images/clean_up_feature.gif' align='center'>\n",
    "\n",
    "### Compute the averge volume and returns \n",
    "<img src='images/average.gif' align='center'>\n",
    "\n",
    "### Dump the dataframe to csv files\n",
    "<img src='images/csv_out.gif' align='center'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case you cannnot follow along, here you can load the tutorial taskgraph from the file. First one is the graph to calculate the return feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b5f62ea12e4a8fadb0a9f4231075a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph = TaskGraph.load_taskgraph('../taskgraphs/get_return_feature.gq.yaml')\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the full graph and click on the `run` button to see the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59ce74a2129648c984af82ce1e736a38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph = TaskGraph.load_taskgraph('../taskgraphs/tutorial_intro.gq.yaml')\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About Task graphs, nodes and plugins\n",
    "\n",
    "Quant processing operators are defined as nodes that operates on **cuDF**/**dask_cuDF** dataframes.\n",
    "\n",
    "A **task graph** is a list of tasks composed of greenflow nodes.\n",
    "\n",
    "The cell below contains the task graph described before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter(\"ignore\")\n",
    "csv_average_return = 'average_return.csv'\n",
    "csv_average_volume = 'average_volume.csv'\n",
    "csv_file_path = './data/stock_price_hist.csv.gz'\n",
    "csv_name_file_path = './data/security_master.csv.gz'\n",
    "from greenflow.dataframe_flow import TaskSpecSchema \n",
    "\n",
    "# load csv stock data\n",
    "task_csvdata = {\n",
    "    TaskSpecSchema.task_id: 'stock_data',\n",
    "    TaskSpecSchema.node_type: 'CsvStockLoader',\n",
    "    TaskSpecSchema.conf: {'file': csv_file_path},\n",
    "    TaskSpecSchema.inputs: {},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.dataloader\"\n",
    "}\n",
    "\n",
    "# filter out the stocks that has average volume smaller than 50\n",
    "task_minVolume = {\n",
    "    TaskSpecSchema.task_id: 'volume_filter',\n",
    "    TaskSpecSchema.node_type: 'ValueFilterNode',\n",
    "    TaskSpecSchema.conf: [{'min': 50.0, 'column': 'volume'}],\n",
    "    TaskSpecSchema.inputs: {'in': 'stock_data.cudf_out'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "# sort the stock symbols and datetime\n",
    "task_sort = {\n",
    "    TaskSpecSchema.task_id: 'sort_node',\n",
    "    TaskSpecSchema.node_type: 'SortNode',\n",
    "    TaskSpecSchema.conf: {'keys': ['asset', 'datetime']},\n",
    "    TaskSpecSchema.inputs: {'in': 'volume_filter.out'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "# add rate of return as a feature into the table\n",
    "task_addReturn = {\n",
    "    TaskSpecSchema.task_id: 'add_return_feature',\n",
    "    TaskSpecSchema.node_type: 'ReturnFeatureNode',\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {'stock_in': 'sort_node.out'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "# read the stock symbol name file and join the computed dataframes\n",
    "task_stockSymbol = {\n",
    "    TaskSpecSchema.task_id: 'stock_name',\n",
    "    TaskSpecSchema.node_type: 'StockNameLoader',\n",
    "    TaskSpecSchema.conf: {'file': csv_name_file_path },\n",
    "    TaskSpecSchema.inputs: {},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.dataloader\"\n",
    "}\n",
    "\n",
    "# In two branches, compute the mean volume and mean return seperately\n",
    "task_volumeMean = {\n",
    "    TaskSpecSchema.task_id: 'average_volume',\n",
    "    TaskSpecSchema.node_type: 'AverageNode',\n",
    "    TaskSpecSchema.conf: {'column': 'volume'},\n",
    "    TaskSpecSchema.inputs: {'stock_in': 'add_return_feature.stock_out'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "task_returnMean = {\n",
    "    TaskSpecSchema.task_id: 'average_return',\n",
    "    TaskSpecSchema.node_type: 'AverageNode',\n",
    "    TaskSpecSchema.conf: {'column': 'returns'},\n",
    "    TaskSpecSchema.inputs: {'stock_in': 'add_return_feature.stock_out'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "task_leftMerge1 = {\n",
    "    TaskSpecSchema.task_id: 'left_merge1',\n",
    "    TaskSpecSchema.node_type: 'LeftMergeNode',\n",
    "    TaskSpecSchema.conf: {'column': 'asset'},\n",
    "    TaskSpecSchema.inputs: {'left': 'average_return.stock_out', \n",
    "                            'right': 'stock_name.stock_name'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "task_leftMerge2 = {\n",
    "    TaskSpecSchema.task_id: 'left_merge2',\n",
    "    TaskSpecSchema.node_type: 'LeftMergeNode',\n",
    "    TaskSpecSchema.conf: {'column': 'asset'},\n",
    "    TaskSpecSchema.inputs: {'left': 'average_volume.stock_out', \n",
    "                            'right': 'stock_name.stock_name'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.transform\"\n",
    "}\n",
    "\n",
    "# output the result in csv files\n",
    "\n",
    "task_outputCsv1 = {\n",
    "    TaskSpecSchema.task_id: 'output_csv1',\n",
    "    TaskSpecSchema.node_type: 'OutCsvNode',\n",
    "    TaskSpecSchema.conf: {'path': csv_average_return},\n",
    "    TaskSpecSchema.inputs: {'df_in': 'left_merge1.merged'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.analysis\"\n",
    "}\n",
    "\n",
    "task_outputCsv2 = {\n",
    "    TaskSpecSchema.task_id: 'output_csv2',\n",
    "    TaskSpecSchema.node_type: 'OutCsvNode',\n",
    "    TaskSpecSchema.conf: {'path': csv_average_volume },\n",
    "    TaskSpecSchema.inputs: {'df_in': 'left_merge2.merged'},\n",
    "    TaskSpecSchema.module: \"greenflow_gquant_plugin.analysis\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, a greenflow task-spec is defined as a dictionary with the following fields:\n",
    "- `id`\n",
    "- `type`\n",
    "- `conf`\n",
    "- `inputs`\n",
    "- `filepath`\n",
    "- `module`\n",
    "\n",
    "As a best practice, we recommend using the `TaskSpecSchema` class for these fields, instead of strings.\n",
    "\n",
    "The `id` for a given task must be unique within a task graph. To use the result(s) of other task(s) as input(s) of a different task, we use the id(s) of the former task(s) in the `inputs` field of the next task.\n",
    "\n",
    "The `type` field contains the node type to use for the compute task. greenflow includes a collection of node classes. These can be found in `greenflow.plugin_nodes`. Click [here](#node_class_example) to see a greenflow node class example.\n",
    "\n",
    "The `conf` field is used to parameterise a task. It lets you access user-set parameters within a plugin (such as `self.conf['min']` in the example above). Each node defines the `conf` json schema. The greenflow UI can use this schema to generate the proper form UI for the inputs. It is recommended to use the UI to configure the `conf`. \n",
    "\n",
    "The `filepath` field is used to specify a python module where a custom plugin is defined. It is optional if the plugin is in `plugin_nodes` directory, and mandatory when the plugin is somewhere else. In a different tutorial, we will learn how to create custom plugins.\n",
    "\n",
    "The `module` is optional to tell greenflow the name of module that the node type is from. If it is not specified, greenflow will search for it among all the customized modules. \n",
    "\n",
    "A custom node schema will look something like this:\n",
    "```\n",
    "custom_task = {\n",
    "    TaskSpecSchema.task_id: 'custom_calc',\n",
    "    TaskSpecSchema.node_type: 'CustomNode',\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: ['some_other_node'],\n",
    "    TaskSpecSchema.filepath: 'custom_nodes.py'\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we compose our task graph and visualize it as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84462c2c25e64a4bb8a1b271adf40773",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from greenflow.dataframe_flow import TaskGraph\n",
    "\n",
    "# list of nodes composing the task graph\n",
    "task_list = [\n",
    "    task_csvdata, task_minVolume, task_sort, task_addReturn,\n",
    "    task_stockSymbol, task_volumeMean, task_returnMean,\n",
    "    task_leftMerge1, task_leftMerge2,\n",
    "    task_outputCsv1, task_outputCsv2]\n",
    "\n",
    "task_graph = TaskGraph(task_list)\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `save_taskgraph` method to save the task graph to a **yaml file**.\n",
    "\n",
    "That will allow us to re-use it in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph_file_name = '01_tutorial_task_graph.gq.yaml'\n",
    "\n",
    "task_graph.save_taskgraph(task_graph_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a snippet of the content in the resulting yaml file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- id: stock_data\n",
      "  type: CsvStockLoader\n",
      "  conf:\n",
      "    file: ./data/stock_price_hist.csv.gz\n",
      "  inputs: {}\n",
      "  module: greenflow_gquant_plugin.dataloader\n",
      "- id: volume_filter\n",
      "  type: ValueFilterNode\n",
      "  conf:\n",
      "  - column: volume\n",
      "    min: 50.0\n",
      "  inputs:\n",
      "    in: stock_data.cudf_out\n",
      "  module: greenflow_gquant_plugin.transform\n",
      "- id: sort_node\n",
      "  type: SortNode\n",
      "  conf:\n",
      "    keys:\n",
      "    - asset\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"$task_graph_file_name\"\n",
    "head -n 19 $1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The yaml file describes the computation tasks.  We can load it and visualize it as a graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9fecec9e8a74dcd906682bd96cecea1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GreenflowWidget(sub=HBox(), value=[OrderedDict([('id', 'stock_data'), ('type', 'CsvStockLoader'), ('conf', {'f…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph = TaskGraph.load_taskgraph(task_graph_file_name)\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a task graph\n",
    "\n",
    "Running the task graph is the next logical step. Nevertheless, it can optionally be built before running it.\n",
    "\n",
    "By calling `build` method, the graph is traversed without running the dataframe computations. This could be useful to inspect the column names and types, validate that the plugins can be instantiated, and check for errors.\n",
    "\n",
    "The output of `build` are instances of each task in a dictionary.\n",
    "\n",
    "In the example below, we inspect the column names and types for the inputs and outputs of the `left_merge1` task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output of build task graph are instances of each task in a dictionary:\n",
      "\n",
      "stock_data: <NodeInTaskGraph greenflow_gquant_plugin.dataloader.csvStockLoader.CsvStockLoader object at 0x7faf857b7d00>\n",
      "volume_filter: <NodeInTaskGraph greenflow_gquant_plugin.transform.valueFilterNode.ValueFilterNode object at 0x7faf857b7d30>\n",
      "sort_node: <NodeInTaskGraph greenflow_gquant_plugin.transform.sortNode.SortNode object at 0x7faf8575d6a0>\n",
      "add_return_feature: <NodeInTaskGraph greenflow_gquant_plugin.transform.returnFeatureNode.ReturnFeatureNode object at 0x7faf8575d730>\n",
      "stock_name: <NodeInTaskGraph greenflow_gquant_plugin.dataloader.stockNameLoader.StockNameLoader object at 0x7faf8575d9d0>\n",
      "average_volume: <NodeInTaskGraph greenflow_gquant_plugin.transform.averageNode.AverageNode object at 0x7faf857b7ee0>\n",
      "average_return: <NodeInTaskGraph greenflow_gquant_plugin.transform.averageNode.AverageNode object at 0x7fb0d953c730>\n",
      "left_merge1: <NodeInTaskGraph greenflow_gquant_plugin.transform.leftMergeNode.LeftMergeNode object at 0x7faf857780d0>\n",
      "left_merge2: <NodeInTaskGraph greenflow_gquant_plugin.transform.leftMergeNode.LeftMergeNode object at 0x7faf857b7100>\n",
      "output_csv1: <NodeInTaskGraph greenflow_gquant_plugin.analysis.outCsvNode.OutCsvNode object at 0x7faf8575dd30>\n",
      "output_csv2: <NodeInTaskGraph greenflow_gquant_plugin.analysis.outCsvNode.OutCsvNode object at 0x7faf8575d880>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "task_graph.build()\n",
    "\n",
    "print('Output of build task graph are instances of each task in a dictionary:\\n')\n",
    "print(str(task_graph))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output meta in outgoing dataframe:\n",
      "\n",
      "MetaData(inports={'left': {}, 'right': {}}, outports={'merged': {'asset': 'int64', 'returns': 'float64', 'asset_name': 'object'}})\n"
     ]
    }
   ],
   "source": [
    "# output meta in 'left_merge_1' node\n",
    "\n",
    "print('output meta in outgoing dataframe:\\n')\n",
    "pprint(task_graph['left_merge1'].meta_setup())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running a task graph\n",
    "\n",
    "To execute the graph computations, we will use the `run` method. If the `Output_Collector` task node is not added to the graph, a output list can be feeded to the run method. The result can be displayed in a rich mode if the `formated` argument is turned on.\n",
    "\n",
    "`run` can also takes an optional `replace` argument which is used and explained later on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "436fb819fbc94830af64aafe0fb6a658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(layout=Layout(border='1px solid black'), outputs=({'output_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = ['stock_data.cudf_out', 'output_csv1.df_out', 'output_csv2.df_out']\n",
    "task_graph.run(outputs=outputs, formated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result can be used as a tuple or dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>asset</th>\n",
       "      <th>volume</th>\n",
       "      <th>asset_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24568</td>\n",
       "      <td>203.002419</td>\n",
       "      <td>ORN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2557</td>\n",
       "      <td>429.953169</td>\n",
       "      <td>EMMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4142</td>\n",
       "      <td>487.567188</td>\n",
       "      <td>RIGL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>869369</td>\n",
       "      <td>172.961884</td>\n",
       "      <td>IBP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>705684</td>\n",
       "      <td>107.933333</td>\n",
       "      <td>USMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4995</th>\n",
       "      <td>4725</td>\n",
       "      <td>1473.216105</td>\n",
       "      <td>UTSI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4996</th>\n",
       "      <td>4136</td>\n",
       "      <td>81.733333</td>\n",
       "      <td>RGCO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4997</th>\n",
       "      <td>795091</td>\n",
       "      <td>73.178571</td>\n",
       "      <td>FRBA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4998</th>\n",
       "      <td>3811</td>\n",
       "      <td>230.635886</td>\n",
       "      <td>ORIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4999</th>\n",
       "      <td>4672</td>\n",
       "      <td>131.653184</td>\n",
       "      <td>UBNK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       asset       volume asset_name\n",
       "0      24568   203.002419        ORN\n",
       "1       2557   429.953169       EMMS\n",
       "2       4142   487.567188       RIGL\n",
       "3     869369   172.961884        IBP\n",
       "4     705684   107.933333       USMD\n",
       "...      ...          ...        ...\n",
       "4995    4725  1473.216105       UTSI\n",
       "4996    4136    81.733333       RGCO\n",
       "4997  795091    73.178571       FRBA\n",
       "4998    3811   230.635886       ORIT\n",
       "4999    4672   131.653184       UBNK\n",
       "\n",
       "[5000 rows x 3 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = task_graph.run(outputs=outputs)\n",
    "csv_data_df, csv_1_df, csv_2_df = result\n",
    "result['output_csv2.df_out']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can profile each of the computation node running time by turning on the profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id:stock_data process time:3.168s\n",
      "id:volume_filter process time:0.021s\n",
      "id:sort_node process time:0.102s\n",
      "id:add_return_feature process time:0.058s\n",
      "id:average_volume process time:0.013s\n",
      "id:average_return process time:0.014s\n",
      "id:stock_name process time:0.008s\n",
      "id:left_merge1 process time:0.002s\n",
      "id:output_csv1 process time:0.015s\n",
      "id:left_merge2 process time:0.002s\n",
      "id:output_csv2 process time:0.014s\n"
     ]
    }
   ],
   "source": [
    "outputs =  ['stock_data.cudf_out', 'output_csv1.df_out', 'output_csv2.df_out']\n",
    "csv_data_df, csv_1_df, csv_2_df = task_graph.run(outputs=outputs, profile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where most of the time is spent on the csv file processing. This is because we have to convert the time string to the proper format via CPU. Let's inspect the content of `csv_1_df` and `csv_2_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv_1_df content:\n",
      "       asset   returns asset_name\n",
      "0     869301 -0.005854      VNRBP\n",
      "1       3159  0.000315       ISBC\n",
      "2       8044  0.000516        SGU\n",
      "3       2123  0.000801       CGNX\n",
      "4      22873 -0.001068       RENN\n",
      "...      ...       ...        ...\n",
      "4995    3518  0.001136       MPWR\n",
      "4996  707774 -0.000417       MODN\n",
      "4997    4856  0.000979       WIRE\n",
      "4998   22461 -0.000243         MY\n",
      "4999    1973 -0.002916       BOCH\n",
      "\n",
      "[5000 rows x 3 columns]\n",
      "\n",
      "csv_2_df content:\n",
      "       asset       volume asset_name\n",
      "0      24568   203.002419        ORN\n",
      "1       2557   429.953169       EMMS\n",
      "2       4142   487.567188       RIGL\n",
      "3     869369   172.961884        IBP\n",
      "4     705684   107.933333       USMD\n",
      "...      ...          ...        ...\n",
      "4995  869374   279.946042       WATT\n",
      "4996  701990   302.973772       FRGI\n",
      "4997   24636   136.807107       SVVC\n",
      "4998    6190  2069.864690        FNF\n",
      "4999   24153   887.397596        DAR\n",
      "\n",
      "[5000 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print('csv_1_df content:')\n",
    "print(csv_1_df)\n",
    "\n",
    "print('\\ncsv_2_df content:')\n",
    "print(csv_2_df)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, please notice that two resulting csv files has been created:\n",
    "- average_return.csv\n",
    "- average_volume.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "csv files created:\n"
     ]
    }
   ],
   "source": [
    "print('\\ncsv files created:')\n",
    "!find . -iname \"*symbol*\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subgraphs\n",
    "\n",
    "A nice feature of task graphs is that we can evaluate any **subgraph**. For instance, if you are only interested in the `average volume` result, you can run only the tasks which are relevant for that computation.\n",
    "\n",
    "If we would not want to re-run tasks, we could also use the `replace` argument of the `run` function with a `load` option.\n",
    "\n",
    "The `replace` argument needs to be a dictionary where each key is the task/node id. The values are a replacement task-spec dictionary (i.e. each key is a spec overload, and its value is what to overload with).\n",
    "\n",
    "In the example below, instead of re-running the `stock_data` node to load a csv file into a `cudf` dataframe, we will use its dataframe output to load from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       asset       volume\n",
      "0      22705    67.929114\n",
      "1     869315   151.844770\n",
      "2       2526    88.337888\n",
      "3       3939    91.674194\n",
      "4     705893  8616.574853\n",
      "...      ...          ...\n",
      "4995  869571   639.127042\n",
      "4996    7842   709.077851\n",
      "4997  701570   110.977778\n",
      "4998  701705   970.310847\n",
      "4999    4859   143.615344\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "replace = {\n",
    "    'stock_data': {\n",
    "        'load': {\n",
    "            'cudf_out': csv_data_df\n",
    "        },\n",
    "        'save': True\n",
    "    }\n",
    "}\n",
    "\n",
    "(volume_mean_df, ) = task_graph.run(outputs=['average_volume.stock_out'],\n",
    "                                    replace=replace)\n",
    "\n",
    "print(volume_mean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a convenience, we can save on disk the checkpoints for any of the nodes, and re-load them if needed. It is only needed to set the save option to `True`. This step will take a while depends on the disk IO speed.\n",
    "\n",
    "In the example above, the `replace` spec directs `run` to save on disk for the `stock_data`. If `load` was boolean then the data would be loaded from disk presuming the data was saved to disk in a prior run.\n",
    "\n",
    "The default directory for saving is `<current_workdir>/.cache/<node_id>.hdf5`.\n",
    "\n",
    "`replace` is also used to override parameters in the tasks. For instance, if we wanted to use the value `40.0` instead `50.0` in the task `volume_filter`, we would do something similar to:\n",
    "```\n",
    "replace_spec = {\n",
    "    'volume_filter': {\n",
    "        'conf': {\n",
    "            'min': 40.0\n",
    "        }\n",
    "    },\n",
    "    'some_task': etc...\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Return mean Dataframe:\n",
      "\n",
      "       asset   returns\n",
      "0      22705  0.001691\n",
      "1     869315  0.000701\n",
      "2       2526  0.002374\n",
      "3       3939  0.052447\n",
      "4     705893  0.000790\n",
      "...      ...       ...\n",
      "4995  869571 -0.002908\n",
      "4996    7842  0.000698\n",
      "4997  701570 -0.004115\n",
      "4998  701705  0.002157\n",
      "4999    4859  0.008666\n",
      "\n",
      "[5000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "replace = {'stock_data': {'load': True},\n",
    "           'average_return': {'save': True}}\n",
    "\n",
    "\n",
    "(return_mean_df, ) = task_graph.run(outputs=['average_return.stock_out'], replace=replace)\n",
    "\n",
    "print('Return mean Dataframe:\\n')\n",
    "print(return_mean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we might want to load the `return_mean_df` from the saved file and evaluate only tasks that we are interested in.\n",
    "\n",
    "In the cells below, we compare different load approaches:\n",
    "- in-memory,\n",
    "- from disk, \n",
    "- and not loading at all.\n",
    "\n",
    "When working interactively, or in situations requiring iterative and explorative task graphs, a significant amount of time is saved by just re-loading the data that do not require to be recalculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using in-memory dataframes for load:\n",
      "CPU times: user 156 ms, sys: 76.8 ms, total: 233 ms\n",
      "Wall time: 263 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Using in-memory dataframes for load:')\n",
    "\n",
    "replace = {'stock_data': {'load':  {\n",
    "            'cudf_out': csv_data_df\n",
    "            }},\n",
    "           'average return': {'load': \n",
    "                              {'stock_out': return_mean_df}}\n",
    "          }\n",
    "\n",
    "_ = task_graph.run(outputs=['output_csv2.df_out'], replace=replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cached dataframes on disk for load:\n",
      "CPU times: user 2.52 s, sys: 488 ms, total: 3.01 s\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Using cached dataframes on disk for load:')\n",
    "\n",
    "replace = {'stock_data': {'load': True},\n",
    "           'average return': {'load': True}}\n",
    "\n",
    "_ = task_graph.run(outputs=['output_csv2.df_out'], replace=replace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-running dataframes calculations instead of using load:\n",
      "CPU times: user 2.49 s, sys: 556 ms, total: 3.04 s\n",
      "Wall time: 3.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('Re-running dataframes calculations instead of using load:')\n",
    "\n",
    "replace = {'stock_data': {'load': True}}\n",
    "\n",
    "_ = task_graph.run(outputs=['output_csv2.df_out'], replace=replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An idiomatic way to save data, if not on disk, or load data, if present on disk, is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.52 s, sys: 459 ms, total: 2.98 s\n",
      "Wall time: 3.01 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "loadsave_csv_data    = 'load' if os.path.isfile('./.cache/stock_data.hdf5') else 'save'\n",
    "loadsave_return_mean = 'load' if os.path.isfile('./.cache/average_return.hdf5') else 'save'\n",
    "\n",
    "replace = {'stock_data': {loadsave_csv_data: True},\n",
    "           'average_return': {loadsave_return_mean: True}}\n",
    "\n",
    "_ = task_graph.run(outputs=['output_csv2.df_out'], replace=replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete temporary files\n",
    "\n",
    "A few cells above, we generated a .yaml file containing the example task graph, and also a couple of CSV files.\n",
    "\n",
    "Let's keep our directory clean, and delete them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$task_graph_file_name\"  \"$csv_average_return\" \"$csv_average_volume\" \n",
    "rm -f $1 $2 $3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='node_class_example'></a>\n",
    "\n",
    "---\n",
    "\n",
    "## Node class example\n",
    "\n",
    "Implementing custom nodes in greenflow is very straighforward.\n",
    "\n",
    "Data scientists only need to override five methods in the parent class `Node`:\n",
    "- `init`\n",
    "- `meta_setup`\n",
    "- `ports_setup`\n",
    "- `conf_schema`\n",
    "- `process`\n",
    "\n",
    "`init` method is usually used to define the required column names\n",
    "\n",
    "`ports_setup` defines the input and output ports for the node\n",
    "\n",
    "`meta_setup` method is used to calculate the output meta name and types.\n",
    "\n",
    "`conf_schema` method is used to define the JSON schema for the node conf so the client can generate the proper UI for it.\n",
    "\n",
    "`process` method takes input dataframes and computes the output dataframe. \n",
    "\n",
    "In this way, dataframes are strongly typed, and errors can be detected early before the time-consuming computation happens.\n",
    "\n",
    "Below, it can be observed `ValueFilterNode` implementation details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class ValueFilterNode(_PortTypesMixin, Node):\n",
      "\n",
      "    def init(self):\n",
      "        _PortTypesMixin.init(self)\n",
      "        self.INPUT_PORT_NAME = 'in'\n",
      "        self.OUTPUT_PORT_NAME = 'out'\n",
      "        port_type = PortsSpecSchema.port_type\n",
      "        self.port_inports = {\n",
      "            self.INPUT_PORT_NAME: {\n",
      "                port_type: [\n",
      "                    \"pandas.DataFrame\", \"cudf.DataFrame\",\n",
      "                    \"dask_cudf.DataFrame\", \"dask.dataframe.DataFrame\"\n",
      "                ]\n",
      "            },\n",
      "        }\n",
      "        self.port_outports = {\n",
      "            self.OUTPUT_PORT_NAME: {\n",
      "                port_type: \"${port:in}\"\n",
      "            }\n",
      "        }\n",
      "        cols_required = {\"asset\": \"int64\"}\n",
      "        addition = {}\n",
      "        self.meta_inports = {\n",
      "            self.INPUT_PORT_NAME: cols_required\n",
      "        }\n",
      "        self.meta_outports = {\n",
      "            self.OUTPUT_PORT_NAME: {\n",
      "                self.META_OP: self.META_OP_ADDITION,\n",
      "                self.META_REF_INPUT: self.INPUT_PORT_NAME,\n",
      "                self.META_DATA: addition\n",
      "            }\n",
      "        }\n",
      "\n",
      "    def update(self):\n",
      "        _PortTypesMixin.update(self)\n",
      "\n",
      "    def meta_setup(self):\n",
      "        return _PortTypesMixin.meta_setup(self)\n",
      "\n",
      "    def ports_setup(self):\n",
      "        return _PortTypesMixin.ports_setup(self)\n",
      "\n",
      "    def conf_schema(self):\n",
      "        json = {\n",
      "            \"title\": \"Value Filter Node configure\",\n",
      "            \"type\": \"array\",\n",
      "            \"description\": \"\"\"Filter the dataframe based on a list of\n",
      "            min/max values.\"\"\",\n",
      "            \"items\": {\n",
      "                \"type\": \"object\",\n",
      "                \"properties\": {\n",
      "                    \"column\":  {\n",
      "                        \"type\": \"string\",\n",
      "                        \"description\": \"dataframe column to be filered on\"\n",
      "                    },\n",
      "                    \"min\": {\n",
      "                        \"type\": \"number\",\n",
      "                        \"description\": \"min value, inclusive\"\n",
      "                    },\n",
      "                    \"max\": {\n",
      "                        \"type\": \"number\",\n",
      "                        \"description\": \"max value, inclusive\"\n",
      "                    }\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "        ui = {}\n",
      "        input_meta = self.get_input_meta()\n",
      "        if self.INPUT_PORT_NAME in input_meta:\n",
      "            col_from_inport = input_meta[self.INPUT_PORT_NAME]\n",
      "            enums = [col for col in col_from_inport.keys()]\n",
      "            json['items']['properties']['column']['enum'] = enums\n",
      "            return ConfSchema(json=json, ui=ui)\n",
      "        else:\n",
      "            return ConfSchema(json=json, ui=ui)\n",
      "\n",
      "    def process(self, inputs):\n",
      "        \"\"\"\n",
      "        filter the dataframe based on a list of min/max values. The node's\n",
      "        conf is a list of column criteria. It defines the column name in\n",
      "        'column`, the min value in `min` and the max value in `max`.\n",
      "\n",
      "        Arguments\n",
      "        -------\n",
      "         inputs: list\n",
      "            list of input dataframes.\n",
      "        Returns\n",
      "        -------\n",
      "        dataframe\n",
      "        \"\"\"\n",
      "\n",
      "        input_df = inputs[self.INPUT_PORT_NAME]\n",
      "        str_list = []\n",
      "        for column_item in self.conf:\n",
      "            column_name = column_item['column']\n",
      "            if 'min' in column_item:\n",
      "                minValue = column_item['min']\n",
      "                str_item = '%s >= %f' % (column_name, minValue)\n",
      "                str_list.append(str_item)\n",
      "            if 'max' in column_item:\n",
      "                maxValue = column_item['max']\n",
      "                str_item = '%s <= %f' % (column_name, maxValue)\n",
      "                str_list.append(str_item)\n",
      "        input_df = input_df.query(\" and \".join(str_list))\n",
      "        return {self.OUTPUT_PORT_NAME: input_df}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "from greenflow_gquant_plugin.transform import ValueFilterNode\n",
    "\n",
    "print(inspect.getsource(ValueFilterNode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
