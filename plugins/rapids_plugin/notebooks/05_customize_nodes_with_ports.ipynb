{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customize your own GPU Kernels in gQuant\n",
    "\n",
    "The gQuant is designed to accelerate quantitive finance workflows on the GPU. The acceleration on GPU is facilitated by using cuDF dataframes organized into a computation graph. The cuDF project is a continously evolving library that provides a pandas-like API. Sometimes the data scientists are facing a few challenges that cannot be easily solved:\n",
    "\n",
    "    1. The quantitative work needs customized logic to manipulate the data, and there are no direct methods within cuDF to support this logic.\n",
    "    2. Each cuDF dataframe method call launches the GPU kernel once. For performance crtical task, it is sometimes required to wrap lots of computation steps together in a single GPU kernel to reduce the kernel launch overheads.\n",
    "\n",
    "The solution is to build customized GPU kernels to implement them. The code and examples below illustrate a variety of approaches to implement customized GPU kernels in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '..')\n",
    "# Load necessary Python modules\n",
    "import sys\n",
    "from gquant.dataframe_flow import TaskSpecSchema, TaskGraph, MetaData\n",
    "from gquant.dataframe_flow import Node, NodePorts, PortsSpecSchema\n",
    "from gquant.dataframe_flow import ConfSchema\n",
    "import cudf\n",
    "import numpy as np\n",
    "from numba import cuda\n",
    "import cupy\n",
    "import math\n",
    "import dask\n",
    "import dask_cudf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a utility function to verify the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(ground_truth, computed):\n",
    "    max_difference = (ground_truth - computed).abs().max()\n",
    "    # print('Max Difference: {}'.format(max_difference))\n",
    "    assert(max_difference < 1e-8)\n",
    "    return max_difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph = TaskGraph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Problem: Calculating the distance of points to the origin\n",
    "\n",
    "The sample problem is to take a list of points in 2-D space and compute their distance to the origin.\n",
    "We start by creating a source `Node` in the graph that generates a cuDF dataframe containing some configurable number of random points. A custom node is defined by inheriting from the `Node` class and overriding methods `init`, `meta_setup`, `ports_setup`, `conf_schema`, `process`.\n",
    "\n",
    "The `ports_setup` must return an instance of `NodePorts` which encapsulates the ports specs. Ports specs are dictionaries with port attributes/options per `PortsSpecSchema`.\n",
    "\n",
    "In the case of the `PointNode` below the input port is an empty dictionary, since no inputs are required, and the output has two ports `points_df_out` and `points_ddf_out`. It can output two types of dataframe frames depends who connects it.\n",
    "\n",
    "The `process` method receives a input dictionary where keys are input ports and values are input data. It return a dictionary where the keys correspond to the output ports. \n",
    "\n",
    "The `meta_setup` is used to compute the output meta information. It returns a dictionary where keys correspond to the output ports.\n",
    "\n",
    "The `conf_schema` is used to define the Node configuration [JSON schema](https://json-schema.org/). gQuantlab UI uses [RJSF](https://github.com/rjsf-team/react-jsonschema-form) project to generate HTML form elements based on the JSON schema. [RJSF playground](https://rjsf-team.github.io/react-jsonschema-form/)  is a good place to learn how to write JSON schema and visualize it. `conf_schema` returns `ConfSchema` which encapsulate the JSON schema and UI schema together.\n",
    "\n",
    "The `column` and `port_types` information sometimes are determined dynamically. gQuant provides a few utility functions to help get dynamical graph information. `self.get_connected_inports()` will return a dictionay where keys are connected inport names and values are inport types. \n",
    "`self.get_input_meta()` will return a dictionary where keys are connected inport names and values are column name/type paris from the parent node. `self.outport_connected(port_name)` method returns a boolean if the output port `port_name` is connected. The `PointNode` uses it to determine what kind of computation it needs to do depending on the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointNode(Node):\n",
    "\n",
    "    def ports_setup(self):\n",
    "        input_ports = {}\n",
    "        output_ports = {\n",
    "            'points_df_out': {\n",
    "                PortsSpecSchema.port_type: cudf.DataFrame\n",
    "            },\n",
    "            'points_ddf_out': {\n",
    "                PortsSpecSchema.port_type: dask_cudf.DataFrame\n",
    "            },\n",
    "        }\n",
    "        return NodePorts(inports=input_ports, outports=output_ports)\n",
    "\n",
    "    def conf_schema(self):\n",
    "        json = {\n",
    "            \"title\": \"PointNode configure\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"npts\":  {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"number of data points\",\n",
    "                    \"minimum\": 10\n",
    "                },\n",
    "                \"npartitions\":  {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"num of partitions in the Dask dataframe\",\n",
    "                    \"minimum\": 1\n",
    "                }\n",
    "\n",
    "            },\n",
    "            \"required\": [\"npts\", \"npartitions\"],\n",
    "        }\n",
    "\n",
    "        ui = {\n",
    "            \"npts\": {\"ui:widget\": \"updown\"},\n",
    "            \"npartitions\": {\"ui:widget\": \"updown\"}\n",
    "        }\n",
    "        return ConfSchema(json=json, ui=ui)\n",
    "\n",
    "    def init(self):\n",
    "        pass\n",
    "        \n",
    "    def meta_setup(self):\n",
    "        columns_out = {\n",
    "            'points_df_out': {\n",
    "                'x': 'float64',\n",
    "                'y': 'float64'\n",
    "            },\n",
    "            'points_ddf_out': {\n",
    "                'x': 'float64',\n",
    "                'y': 'float64'\n",
    "            }\n",
    "        }\n",
    "        return MetaData(inports={}, outports=columns_out)\n",
    "\n",
    "    def process(self, inputs):\n",
    "        npts = self.conf['npts']\n",
    "        df = cudf.DataFrame()\n",
    "        df['x'] = np.random.rand(npts)\n",
    "        df['y'] = np.random.rand(npts)\n",
    "        output = {}\n",
    "        if self.outport_connected('points_df_out'):\n",
    "            output.update({'points_df_out': df})\n",
    "        if self.outport_connected('points_ddf_out'):\n",
    "            npartitions = self.conf['npartitions']\n",
    "            ddf = dask_cudf.from_cudf(df, npartitions=npartitions)\n",
    "            output.update({'points_ddf_out': ddf})\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distance can be computed via cuDF methods. We define the `DistanceNode` to calculate the euclidean distance and add a `distance_cudf` column to the output dataframe. We will use that as the ground truth to compare and verify results later. Additionally, the distance node calculates absolute distance (Manhattan distance) in another output port. The compuation is done depending which output is connected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistanceNode(Node):\n",
    "\n",
    "    def ports_setup(self):\n",
    "        port_type = PortsSpecSchema.port_type\n",
    "        input_ports = {\n",
    "            'points_df_in': {\n",
    "                port_type: [cudf.DataFrame, dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_ports = {\n",
    "            'distance_df': {\n",
    "                port_type: [cudf.DataFrame, dask_cudf.DataFrame]\n",
    "            },\n",
    "            'distance_abs_df': {\n",
    "                PortsSpecSchema.port_type:  [cudf.DataFrame, dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "        input_connections = self.get_connected_inports()\n",
    "        if 'points_df_in' in input_connections:\n",
    "            types = input_connections['points_df_in']\n",
    "            # connected, use the types passed in from parent\n",
    "            return NodePorts(inports={'points_df_in': {port_type: types}},\n",
    "                             outports={'distance_df': {port_type: types},\n",
    "                                       'distance_abs_df': {port_type: types},\n",
    "                                       })\n",
    "        else:\n",
    "            return NodePorts(inports=input_ports, outports=output_ports)\n",
    "\n",
    "    def conf_schema(self):\n",
    "        return ConfSchema()\n",
    "\n",
    "    def init(self):\n",
    "        self.delayed_process = True\n",
    "\n",
    "\n",
    "    def meta_setup(self):\n",
    "        req_cols = {\n",
    "            'x': 'float64',\n",
    "            'y': 'float64'\n",
    "        }\n",
    "        required = {\n",
    "            'points_df_in': req_cols,\n",
    "        }\n",
    "        input_meta = self.get_input_meta()\n",
    "        output_cols = ({\n",
    "                'distance_df': {\n",
    "                    'distance_cudf': 'float64',\n",
    "                    'x': 'float64',\n",
    "                    'y': 'float64'\n",
    "                },\n",
    "                'distance_abs_df': {\n",
    "                    'distance_abs_cudf': 'float64',\n",
    "                    'x': 'float64',\n",
    "                    'y': 'float64'\n",
    "                }\n",
    "            })\n",
    "        if 'points_df_in' in input_meta:\n",
    "            col_from_inport = input_meta['points_df_in']\n",
    "            # additional ports\n",
    "            output_cols['distance_df'].update(col_from_inport)\n",
    "            output_cols['distance_abs_df'].update(col_from_inport)\n",
    "        return MetaData(inports=required, outports=output_cols)\n",
    "\n",
    "    def process(self, inputs):\n",
    "        df = inputs['points_df_in']\n",
    "        output = {}\n",
    "        if self.outport_connected('distance_df'):\n",
    "            copy_df = df.copy()\n",
    "            copy_df['distance_cudf'] = (df['x'] ** 2 + df['y'] ** 2).sqrt()\n",
    "            output.update({'distance_df': copy_df})\n",
    "        if self.outport_connected('distance_abs_df'):\n",
    "            copy_df = df.copy()\n",
    "            copy_df['distance_abs_cudf'] = df['x'].abs() + df['y'].abs()\n",
    "            output.update({'distance_abs_df': copy_df})\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having these two nodes, we can construct a simple task graph to compute the distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task specifications.\n",
    "module_name = 'custom_nodes'\n",
    "\n",
    "points_tspec = {\n",
    "    TaskSpecSchema.task_id: 'points_task',\n",
    "    TaskSpecSchema.node_type: PointNode,\n",
    "    TaskSpecSchema.conf: {'npts': 1000},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {},\n",
    "}\n",
    "\n",
    "cudf_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_cudf',\n",
    "    TaskSpecSchema.node_type: DistanceNode,\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'points_task.points_df_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "out_spec = {\n",
    "    TaskSpecSchema.task_id: '',\n",
    "    TaskSpecSchema.node_type: \"Output_Collector\",\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'in0': 'points_task.points_df_out',\n",
    "        'in1': 'distance_by_cudf.distance_df',\n",
    "        'in2': 'distance_by_cudf.distance_abs_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "task_list = [points_tspec, cudf_distance_tspec, out_spec]\n",
    "task_graph = TaskGraph(task_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw the graph in an interactive widget. First, let's register the dynamically defined gQuant nodes so the client knows about them. Note, this step is only needed if we would like to interact with gQuant by Jupyterlab UI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda141664aac494c91f44a21cbc7f594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TaskGraph.register_lab_node(module_name, PointNode)\n",
    "TaskGraph.register_lab_node(module_name, DistanceNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Draw the widget:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef693df518934171a12420bd81a1983f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'points_task'), ('type', 'PointNode'), ('conf', {'npts': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to run the task graph to obtain the distances. The output is identified by the `id` of the distance node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d35ba7ed8ae4f7cb3f46a6d59550f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(layout=Layout(border='1px solid black'), outputs=({'output_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "r = task_graph.run()\n",
    "points_df = r['points_task.points_df_out']\n",
    "task_graph.run(formated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Kernel with Numba library\n",
    "\n",
    "Numba is an excellent python library used for accelerating numerical computations. Numba supports CUDA GPU programming by directly compiling a restricted subset of Python code into CUDA kernels and device functions. The Numba GPU kernel is written in Python and translated (JIT just-in-time compiled) into GPU code at runtime. This is achieved by decorating a Python function with `@cuda.jit`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like a C/C++ CUDA GPU kernel, the `distance_kernel` function is called by thousands of threads in the GPU. The thread id is computed by `threadIdx.x`, `blockId.x` and `blockDim.x` built-in variables. Please check the [CUDA programming guild](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#thread-hierarchy) for details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A cuDF series can be converted to GPU arrays compatible with the Numba library via.to_gpu_array` API. The next step is to define a Node that calls this Numba kernel to compute the distance and save the result into `distance_numba` column in the output dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cupy\n",
    "@cuda.jit\n",
    "def distance_kernel(x, y, distance, array_len):\n",
    "    # ii - overall thread index\n",
    "    ii = cuda.threadIdx.x + cuda.blockIdx.x * cuda.blockDim.x\n",
    "    if ii < array_len:\n",
    "        distance[ii] = math.sqrt(x[ii] ** 2 + y[ii] ** 2)\n",
    "\n",
    "\n",
    "class NumbaDistanceNode(Node):\n",
    "\n",
    "    def ports_setup(self):\n",
    "        port_type = PortsSpecSchema.port_type\n",
    "        input_ports = {\n",
    "            'points_df_in': {\n",
    "                port_type: [cudf.DataFrame,\n",
    "                            dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_ports = {\n",
    "            'distance_df': {\n",
    "                port_type: [cudf.DataFrame,\n",
    "                            dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        input_connections = self.get_connected_inports()\n",
    "        if 'points_df_in' in input_connections:\n",
    "            types = input_connections['points_df_in']\n",
    "            # connected\n",
    "            return NodePorts(inports={'points_df_in': {port_type: types}},\n",
    "                             outports={'distance_df': {port_type: types}})\n",
    "        else:\n",
    "            return NodePorts(inports=input_ports, outports=output_ports)\n",
    "  \n",
    "    def init(self):\n",
    "        self.delayed_process = True\n",
    "\n",
    "\n",
    "    def meta_setup(self,):\n",
    "        required_cols = {'x': 'float64',\n",
    "                    'y': 'float64'}\n",
    "        required = {\n",
    "            'points_df_in': required_cols,\n",
    "            'distance_df': required_cols\n",
    "        }\n",
    "        input_meta = self.get_input_meta()\n",
    "        output_cols = ({\n",
    "                'distance_df': {\n",
    "                    'distance_numba': 'float64',\n",
    "                    'x': 'float64',\n",
    "                    'y': 'float64'\n",
    "                }\n",
    "            })\n",
    "        if 'points_df_in' in input_meta:\n",
    "            col_from_inport = input_meta['points_df_in']\n",
    "            # additional ports\n",
    "            output_cols['distance_df'].update(col_from_inport)\n",
    "        return MetaData(inports=required, outports=output_cols)\n",
    "\n",
    "    def conf_schema(self):\n",
    "        return ConfSchema()\n",
    "\n",
    "    def process(self, inputs):\n",
    "        df = inputs['points_df_in']\n",
    "        number_of_threads = 16\n",
    "        number_of_blocks = ((len(df) - 1) // number_of_threads) + 1\n",
    "        # Inits device array by setting 0 for each index.\n",
    "        # df['distance_numba'] = 0.0\n",
    "        darr = cupy.zeros(len(df))\n",
    "        distance_kernel[(number_of_blocks,), (number_of_threads,)](\n",
    "            df['x'],\n",
    "            df['y'],\n",
    "            darr,\n",
    "            len(df))\n",
    "        df['distance_numba'] = darr\n",
    "        return {'distance_df': df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `self.delayed_process = True` flag in the `meta_setup` is necesary to enable the logic in the `Node` class for handling `dask_cudf` dataframes in order to use Dask (for distributed computation i.e. multi-gpu in examples later on). The `dask_cudf` dataframe does not support GPU customized kernels directly. The `to_delayed` and `from_delayed` low level interfaces of `dask_cudf` enable this support. The gQuant framework handles `dask_cudf` dataframes automatically under the hood when we set this flag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Customized Kernel by CuPy library\n",
    "\n",
    "CuPy is an alternative to Numba. Numba JIT compiles Python code into GPU device code at runtime. There are some limitations in how Numba can be used as well as JIT compilation latency overhead. When a Python process calls a Numba GPU kernel for the first time Numba has to compile the Python code, and each time a new Python process is started the GPU kernel has to be recompiled. If advanced features of CUDA are needed and latency is important, CuPy is an alternative library that can be used to compile C/C++ CUDA code. CuPy caches the GPU device code on disk (default location `$(HOME)/.cupy/kernel_cache` which can be changed via `CUPY_CACHE_DIR` environment variable) thus eliminating compilation latency for subsequent Python processes.\n",
    "\n",
    "`CuPy` GPU kernel is esentially a C/C++ GPU kernel. Below we define the `compute_distance` kernel using `CuPy`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using gQuant we can now define a Node that calls this CuPy kernel to compute the distance and save the results into `distance_cupy` column of a `cudf` dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_string = r'''\n",
    "    extern \"C\" __global__\n",
    "    void compute_distance(const double* x, const double* y,\n",
    "            double* distance, int arr_len) {\n",
    "        int tid = blockDim.x * blockIdx.x + threadIdx.x;\n",
    "        if (tid < arr_len){\n",
    "        distance[tid] = sqrt(x[tid]*x[tid] + y[tid]*y[tid]);\n",
    "        }\n",
    "    }\n",
    "'''\n",
    "\n",
    "\n",
    "class CupyDistanceNode(Node):\n",
    "\n",
    "    def ports_setup(self):\n",
    "        port_type = PortsSpecSchema.port_type\n",
    "        input_ports = {\n",
    "            'points_df_in': {\n",
    "                port_type: [cudf.DataFrame,\n",
    "                            dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_ports = {\n",
    "            'distance_df': {\n",
    "                port_type: [cudf.DataFrame,\n",
    "                            dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "\n",
    "        input_connections = self.get_connected_inports()\n",
    "        if 'points_df_in' in input_connections:\n",
    "            types = input_connections['points_df_in']\n",
    "            # connected\n",
    "            return NodePorts(inports={'points_df_in': {port_type: types}},\n",
    "                             outports={'distance_df': {port_type: types}})\n",
    "        else:\n",
    "            return NodePorts(inports=input_ports, outports=output_ports)\n",
    "\n",
    "    def init(self):\n",
    "        self.delayed_process = True\n",
    "\n",
    "\n",
    "    def meta_setup(self,):\n",
    "        cols_required = {'x': 'float64',\n",
    "                         'y': 'float64'}\n",
    "        required = {\n",
    "            'points_df_in': cols_required,\n",
    "            'distance_df': cols_required\n",
    "        }\n",
    "        input_meta = self.get_input_meta()\n",
    "        output_cols = ({\n",
    "                'distance_df': {\n",
    "                    'distance_cupy': 'float64',\n",
    "                    'x': 'float64',\n",
    "                    'y': 'float64'\n",
    "                }\n",
    "            })\n",
    "        if 'points_df_in' in input_meta:\n",
    "            col_from_inport = input_meta['points_df_in']\n",
    "            # additional ports\n",
    "            output_cols['distance_df'].update(col_from_inport)\n",
    "        return MetaData(inports=required, outports=output_cols)\n",
    "\n",
    "    def conf_schema(self):\n",
    "        return ConfSchema()\n",
    "\n",
    "    def get_kernel(self):\n",
    "        raw_kernel = cupy.RawKernel(kernel_string, 'compute_distance')\n",
    "        return raw_kernel\n",
    "\n",
    "    def process(self, inputs):\n",
    "        df = inputs['points_df_in']\n",
    "        cupy_x = cupy.asarray(df['x'])\n",
    "        cupy_y = cupy.asarray(df['y'])\n",
    "        number_of_threads = 16\n",
    "        number_of_blocks = (len(df) - 1) // number_of_threads + 1\n",
    "        dis = cupy.ndarray(len(df), dtype=cupy.float64)\n",
    "        self.get_kernel()((number_of_blocks,), (number_of_threads,),\n",
    "                          (cupy_x, cupy_y, dis, len(df)))\n",
    "        df['distance_cupy'] = dis\n",
    "\n",
    "        return {'distance_df': df}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `self.delayed_process = True` flag is added for the same reason as with `DistanceNumbaNode` i.e. to support `dask_cudf` data frames.\n",
    "\n",
    "Let's register these two added new nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskGraph.register_lab_node(module_name, NumbaDistanceNode)\n",
    "TaskGraph.register_lab_node(module_name, CupyDistanceNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing using the Nodes with customized GPU kernels\n",
    "\n",
    "First we construct the computation graph for gQuant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5321bc21c3f141d58f2e0a3d5db18de5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'points_task'), ('type', 'PointNode'), ('conf', {'npts': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For comparison to above re-use points dataframe instead\n",
    "# of rand generating each time when running the task-graph.\n",
    "\n",
    "numba_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_numba',\n",
    "    TaskSpecSchema.node_type: NumbaDistanceNode,\n",
    "    TaskSpecSchema.conf: {}, \n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'points_task.points_df_out'\n",
    "    },\n",
    "}\n",
    "\n",
    "cupy_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_cupy',\n",
    "    TaskSpecSchema.node_type: CupyDistanceNode,\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'points_task.points_df_out'\n",
    "    },\n",
    "}\n",
    "\n",
    "out_spec = {\n",
    "    TaskSpecSchema.task_id: '',\n",
    "    TaskSpecSchema.node_type: \"Output_Collector\",\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'in0': 'distance_by_cudf.distance_df',\n",
    "        'in1': 'distance_by_numba.distance_df',\n",
    "        'in2': 'distance_by_cupy.distance_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "task_list = [\n",
    "    points_tspec,\n",
    "    cudf_distance_tspec,\n",
    "    numba_distance_tspec,\n",
    "    cupy_distance_tspec,\n",
    "    out_spec\n",
    "]\n",
    "task_graph = TaskGraph(task_list)\n",
    "\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or, run it programmatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e0df416909d45aa8387613af29f32fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(layout=Layout(border='1px solid black'), outputs=({'output_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out_list = [\n",
    "    'distance_by_cudf.distance_df',\n",
    "    'distance_by_numba.distance_df',\n",
    "    'distance_by_cupy.distance_df'\n",
    "]\n",
    "cache_load = {\"points_task\": {\"load\": {'points_df_out': points_df}}}\n",
    "(df_w_cudf, df_w_numba, df_w_cupy) = task_graph.run(out_list, replace=cache_load)\n",
    "task_graph.run(out_list, replace=cache_load, formated=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `verify` function defined above to verify the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Difference cudf to numba: 2.220446049250313e-16\n",
      "Max Difference cudf to cupy: 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "mdiff = verify(df_w_cudf['distance_cudf'], df_w_numba['distance_numba'])\n",
    "print('Max Difference cudf to numba: {}'.format(mdiff))\n",
    "mdiff = verify(df_w_cudf['distance_cudf'], df_w_cupy['distance_cupy'])\n",
    "print('Max Difference cudf to cupy: {}'.format(mdiff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate multi-input nodes let's create a verify node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VerifyNode(Node):\n",
    "\n",
    "    def ports_setup(self):\n",
    "        input_ports = {\n",
    "            'df1': {\n",
    "                PortsSpecSchema.port_type: [cudf.DataFrame,\n",
    "                                            dask_cudf.DataFrame]\n",
    "            },\n",
    "            'df2': {\n",
    "                PortsSpecSchema.port_type: [cudf.DataFrame,\n",
    "                                            dask_cudf.DataFrame]\n",
    "            }\n",
    "        }\n",
    "        output_ports = {\n",
    "            'max_diff': {\n",
    "                PortsSpecSchema.port_type: float\n",
    "            }\n",
    "        }\n",
    "\n",
    "        connections = self.get_connected_inports()   \n",
    "        for key in input_ports:\n",
    "            if key in connections:\n",
    "                # connected\n",
    "                types = connections[key]\n",
    "                input_ports[key].update({PortsSpecSchema.port_type: types})\n",
    "        return NodePorts(inports=input_ports, outports=output_ports)\n",
    "\n",
    "    def meta_setup(self):\n",
    "        required ={\n",
    "            \"df1\": {},\n",
    "            \"df2\": {}\n",
    "        }\n",
    "        return MetaData(inports=required, outports={'max_diff': {}})\n",
    "\n",
    "    def conf_schema(self):\n",
    "        json = {\n",
    "            \"title\": \"VerifyNode configure\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"df1_col\":  {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"dataframe1 column name\"\n",
    "                },\n",
    "                \"df2_col\":  {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"dataframe2 column name\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"df1_col\", \"df2_col\"],\n",
    "        }\n",
    "\n",
    "        ui = {\n",
    "            \"df1_col\": {\"ui:widget\": \"text\"},\n",
    "            \"df2_col\": {\"ui:widget\": \"text\"}\n",
    "        }\n",
    "        return ConfSchema(json=json, ui=ui)\n",
    "\n",
    "    def process(self, inputs):\n",
    "        df1 = inputs['df1']\n",
    "        df2 = inputs['df2']\n",
    "        col_df1 = self.conf['df1_col']\n",
    "        col_df2 = self.conf['df2_col']\n",
    "\n",
    "        df1_col = df1[col_df1]\n",
    "        if isinstance(df1, dask_cudf.DataFrame):\n",
    "            # df1_col = df1_col.compute()\n",
    "            pass\n",
    "\n",
    "        df2_col = df2[col_df2]\n",
    "        if isinstance(df2, dask_cudf.DataFrame):\n",
    "            # df2_col = df2_col.compute()\n",
    "            pass\n",
    "\n",
    "        max_difference = (df1_col - df2_col).abs().max()\n",
    "\n",
    "        if isinstance(max_difference, dask.dataframe.core.Scalar):\n",
    "            max_difference = float(max_difference.compute())\n",
    "        max_difference = float(max_difference)\n",
    "        return {'max_diff': max_difference}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register the `VerifyNode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskGraph.register_lab_node(module_name, VerifyNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the full Taskgraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69a48776885e438abe2a3129d1cf0bac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'points_task'), ('type', 'PointNode'), ('conf', {'npts': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "verify_tspec = {\n",
    "    TaskSpecSchema.task_id: 'verify_cudf_to_numba',\n",
    "    TaskSpecSchema.node_type: VerifyNode,\n",
    "    TaskSpecSchema.conf: {\n",
    "        'df1_col': 'distance_cudf',\n",
    "        'df2_col': 'distance_numba'\n",
    "    },    \n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'df1': 'distance_by_cudf.distance_df',\n",
    "        'df2': 'distance_by_numba.distance_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "verify_tspec2 = {\n",
    "    TaskSpecSchema.task_id: 'verify_cudf_to_cupy',\n",
    "    TaskSpecSchema.node_type: VerifyNode,\n",
    "    TaskSpecSchema.conf: {\n",
    "        'df1_col': 'distance_cudf',\n",
    "        'df2_col': 'distance_cupy'\n",
    "    },\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'df1': 'distance_by_cudf.distance_df',\n",
    "        'df2': 'distance_by_cupy.distance_df'\n",
    "    }\n",
    "}\n",
    "out_spec = {\n",
    "    TaskSpecSchema.task_id: '',\n",
    "    TaskSpecSchema.node_type: \"Output_Collector\",\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'in0': 'verify_cudf_to_numba.max_diff',\n",
    "        'in1': 'verify_cudf_to_cupy.max_diff'\n",
    "    }\n",
    "}\n",
    "\n",
    "task_list = [\n",
    "    points_tspec,\n",
    "    cudf_distance_tspec,\n",
    "    numba_distance_tspec,\n",
    "    cupy_distance_tspec,\n",
    "    out_spec,\n",
    "    verify_tspec, \n",
    "    verify_tspec2\n",
    "]\n",
    "task_graph = TaskGraph(task_list)\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Difference cudf to numba: 2.220446049250313e-16\n",
      "Max Difference cudf to cupy: 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "(max_cudf_to_numba_diff, max_cudf_to_cupy_diff) = task_graph.run([\n",
    "    'verify_cudf_to_numba.max_diff',\n",
    "    'verify_cudf_to_cupy.max_diff'\n",
    "])\n",
    "print('Max Difference cudf to numba: {}'.format(max_cudf_to_numba_diff))\n",
    "print('Max Difference cudf to cupy: {}'.format(max_cudf_to_cupy_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask distributed computation\n",
    "\n",
    "Using Dask and `dask-cudf` we can run the Nodes with customized GPU kernels on distributed dataframes. Under the hood of the `Node` class the Dask delayed processing API is handled for cudf dataframes when the `self.delayed_process = True` flag is set.\n",
    "\n",
    "We first start a distributed Dask environment. When a dask client is instantiated it registers itself as the default Dask scheduler (<http://distributed.dask.org/en/latest/client.html>). Therefore all subsequent Dask distibuted dataframe operations will run in distributed fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:35911</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>2</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>100.00 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:35911' processes=2 threads=2, memory=100.00 GB>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dask_cuda import LocalCUDACluster\n",
    "from dask.distributed import Client\n",
    "\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Dask status page can be displayed in a web browser at `<ip-address>:8787`. The ip-address corresponds to the machine where the dask cluster (scheduler) was launched. Most likely same ip-address as where this jupyter notebook is running. Using the Dask status page is convenient for monitoring dask distributed processing. <http://distributed.dask.org/en/latest/web.html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to partition the `cudf` dataframe into a `dask_cudf` dataframe. Here we make the number of partitions corresponding to the number of workers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistributedNode(Node):\n",
    "\n",
    "    def ports_setup(self):\n",
    "        input_ports = {\n",
    "            'points_df_in': {\n",
    "                PortsSpecSchema.port_type: cudf.DataFrame\n",
    "            }\n",
    "        }\n",
    "\n",
    "        output_ports = {\n",
    "            'points_ddf_out': {\n",
    "                PortsSpecSchema.port_type: dask_cudf.DataFrame\n",
    "            }\n",
    "        }\n",
    "\n",
    "        return NodePorts(inports=input_ports, outports=output_ports)\n",
    "\n",
    "    def init(self):\n",
    "        pass\n",
    "\n",
    "    def meta_setup(self,):\n",
    "        cols_required = {\n",
    "            'x': 'float64',\n",
    "            'y': 'float64'\n",
    "        }\n",
    "        required = {\n",
    "            'points_df_in': cols_required,\n",
    "            'points_ddf_out': cols_required\n",
    "        }\n",
    "        input_meta = self.get_input_meta()\n",
    "        output_cols = ({\n",
    "                'points_ddf_out': {\n",
    "                    'x': 'float64',\n",
    "                    'y': 'float64'\n",
    "                }\n",
    "            })\n",
    "        if 'points_df_in' in input_meta:\n",
    "            col_from_inport = input_meta['points_df_in']\n",
    "            # additional ports\n",
    "            output_cols['points_ddf_out'].update(col_from_inport)\n",
    "        return MetaData(inports=required, outports=output_cols)\n",
    "\n",
    "    def conf_schema(self):\n",
    "        json = {\n",
    "            \"title\": \"DistributedNode configure\",\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"npartitions\":  {\n",
    "                    \"type\": \"number\",\n",
    "                    \"description\": \"num of partitions in the Dask dataframe\",\n",
    "                    \"minimum\": 1\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"npartitions\"],\n",
    "        }\n",
    "\n",
    "        ui = {\n",
    "            \"npartitions\": {\"ui:widget\": \"updown\"}\n",
    "        }\n",
    "        return ConfSchema(json=json, ui=ui)\n",
    "\n",
    "    def process(self, inputs):\n",
    "        npartitions = self.conf['npartitions']\n",
    "        df = inputs['points_df_in']\n",
    "        ddf = dask_cudf.from_cudf(df, npartitions=npartitions)\n",
    "        return {'points_ddf_out': ddf}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Register it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TaskGraph.register_lab_node(module_name, DistributedNode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add this distribution node to the computation graph to convert `cudf` dataframes into `dask-cudf` dataframes. The `dask-cudf` dataframes are handled automatically in gQuant when `self.delayed_process=True` within a `Node` implementation (setup in `meta_setup`). When using nodes with ports with `self.delayed_process=True` setting, it is required that all input and output ports be of type `cudf.DataFrame`. Otherwise don't set  `self.delayed_process` and one can write custom logic to handle distributed dataframes (refer to `VerifyNode` abover for an example where `dask_cudf` dataframes are handled directly within the process method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3409e93edddf4fc1ab9a4fbe7ccc5728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'points_task'), ('type', 'PointNode'), ('conf', {'npts': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "npartitions = len(client.scheduler_info()['workers'])\n",
    "\n",
    "\n",
    "distribute_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distributed_points',\n",
    "    TaskSpecSchema.node_type: DistributedNode,\n",
    "    TaskSpecSchema.conf: {'npartitions': npartitions},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'points_task.points_df_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "dask_cudf_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_cudf',\n",
    "    TaskSpecSchema.node_type: DistanceNode,\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'distributed_points.points_ddf_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "dask_numba_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_numba',\n",
    "    TaskSpecSchema.node_type: NumbaDistanceNode,\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'distributed_points.points_ddf_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "dask_cupy_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_cupy',\n",
    "    TaskSpecSchema.node_type: CupyDistanceNode,\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.module: module_name,\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'distributed_points.points_ddf_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "out_spec = {\n",
    "    TaskSpecSchema.task_id: '',\n",
    "    TaskSpecSchema.node_type: \"Output_Collector\",\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'in0': 'distributed_points.points_ddf_out',\n",
    "        'in1': 'distance_by_cudf.distance_df',\n",
    "        'in2': 'distance_by_numba.distance_df',\n",
    "        'in3': 'distance_by_cupy.distance_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "task_list = [\n",
    "    points_tspec,\n",
    "    distribute_tspec,\n",
    "    dask_cudf_distance_tspec,\n",
    "    dask_numba_distance_tspec,\n",
    "    dask_cupy_distance_tspec,\n",
    "    out_spec\n",
    "]\n",
    "\n",
    "task_graph = TaskGraph(task_list)\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the distributed computation programmatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_list = [\n",
    "    'distributed_points.points_ddf_out',\n",
    "    'distance_by_cudf.distance_df',\n",
    "    'distance_by_numba.distance_df',\n",
    "    'distance_by_cupy.distance_df'\n",
    "]\n",
    "\n",
    "(points_ddf, ddf_w_cudf, ddf_w_numba, ddf_w_cupy) = task_graph.run(out_list)\n",
    "df_w_cudf = ddf_w_cudf.compute()\n",
    "df_w_numba = ddf_w_numba.compute()\n",
    "df_w_cupy = ddf_w_cupy.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3409e93edddf4fc1ab9a4fbe7ccc5728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(cache={'nodes': [{'width': 110, 'id': 'points_task', 'type': 'PointNode', 'schema': {'title': 'Po…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "verify_cudf_numba_tspec = verify_tspec.copy()\n",
    "verify_cudf_cupy_tspec = verify_tspec2.copy()\n",
    "\n",
    "task_graph.extend(\n",
    "    [verify_cudf_numba_tspec,\n",
    "     verify_cudf_cupy_tspec],\n",
    "    replace=True)\n",
    "task_graph.draw()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD points_ddf:\n",
      "          x         y\n",
      "0  0.330374  0.558295\n",
      "1  0.017715  0.185780\n",
      "2  0.779221  0.601872\n",
      "3  0.619049  0.466921\n",
      "4  0.118439  0.426810\n",
      "\n",
      "HEAD df_w_cudf:\n",
      "          x         y  distance_cudf\n",
      "0  0.330374  0.558295       0.648722\n",
      "1  0.017715  0.185780       0.186623\n",
      "2  0.779221  0.601872       0.984599\n",
      "3  0.619049  0.466921       0.775395\n",
      "4  0.118439  0.426810       0.442938\n",
      "\n",
      "HEAD df_w_numba:\n",
      "          x         y  distance_numba\n",
      "0  0.330374  0.558295        0.648722\n",
      "1  0.017715  0.185780        0.186623\n",
      "2  0.779221  0.601872        0.984599\n",
      "3  0.619049  0.466921        0.775395\n",
      "4  0.118439  0.426810        0.442938\n",
      "\n",
      "HEAD df_w_cupy:\n",
      "          x         y  distance_cupy\n",
      "0  0.330374  0.558295       0.648722\n",
      "1  0.017715  0.185780       0.186623\n",
      "2  0.779221  0.601872       0.984599\n",
      "3  0.619049  0.466921       0.775395\n",
      "4  0.118439  0.426810       0.442938\n",
      "\n",
      "Max Difference cudf to numba: 2.220446049250313e-16\n",
      "Max Difference cudf to cupy: 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "# Use results above and avoid re-running dask\n",
    "replace_spec = {\n",
    "    'distance_by_cudf': {\n",
    "        TaskSpecSchema.load: {\n",
    "            'distance_df': ddf_w_cudf\n",
    "        }\n",
    "    },\n",
    "    'distance_by_numba': {\n",
    "        TaskSpecSchema.load: {\n",
    "            'distance_df': ddf_w_numba\n",
    "        }\n",
    "    },\n",
    "    'distance_by_cupy': {\n",
    "        TaskSpecSchema.load: {\n",
    "            'distance_df': ddf_w_cupy\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "(max_cudf_to_numba_diff, max_cudf_to_cupy_diff) = task_graph.run(\n",
    "    ['verify_cudf_to_numba.max_diff',\n",
    "     'verify_cudf_to_cupy.max_diff'],\n",
    "    replace=replace_spec\n",
    ")\n",
    "\n",
    "print('HEAD points_ddf:\\n{}\\n'.format(points_ddf.head()))\n",
    "print('HEAD df_w_cudf:\\n{}\\n'.format(ddf_w_cudf.head()))\n",
    "print('HEAD df_w_numba:\\n{}\\n'.format(ddf_w_numba.head()))\n",
    "print('HEAD df_w_cupy:\\n{}\\n'.format(ddf_w_cupy.head()))\n",
    "print('Max Difference cudf to numba: {}'.format(max_cudf_to_numba_diff))\n",
    "print('Max Difference cudf to cupy: {}'.format(max_cudf_to_cupy_diff))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One limitation to be aware of when using customized kernels within Nodes in the Dask environment, is that each GPU kernel works on one partition of the dataframe. Therefore if the computation depends on other partitions of the dataframe the approach above does not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving Custom Nodes and Kernels\n",
    "\n",
    "The gQuant examples already implement a number of `Nodes`. These can be found in `gquant.plugin_nodes` submodules.\n",
    "\n",
    "The customized kernels and nodes can be saved to your own python modules for future re-use instead of having to re-define them at runtime. The nodes we defined above were to a written to a python module \"custom_port_nodes.py\" (the `DistanceNode` was simplified to ommit the absolute distance calculation). We will re-run our workflow importing the Nodes from the custom module we wrote out.\n",
    "\n",
    "When defining the tasks we specify `filepath` for the path to the python module that has the Node definition. Notice, that the `node_type` is specified as a string instead of class. The string is the class name of the node that will be imported for running a task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9223197dbc4c28879b53737e9fe607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'points_task'), ('type', 'PointNode'), ('conf', {'npts': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gquant.dataframe_flow.util import get_file_path\n",
    "npartitions = len(client.scheduler_info()['workers'])\n",
    "points_tspec = {\n",
    "    TaskSpecSchema.task_id: 'points_task',\n",
    "    TaskSpecSchema.node_type: 'PointNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {'npts': 1000},\n",
    "    TaskSpecSchema.inputs: {},\n",
    "}\n",
    "\n",
    "distribute_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distributed_points',\n",
    "    TaskSpecSchema.node_type: 'DistributedNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {'npartitions': npartitions},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'points_task.points_df_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "dask_cudf_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_cudf',\n",
    "    TaskSpecSchema.node_type: 'DistanceNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'distributed_points.points_ddf_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "dask_numba_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_numba',\n",
    "    TaskSpecSchema.node_type: 'NumbaDistanceNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'distributed_points.points_ddf_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "dask_cupy_distance_tspec = {\n",
    "    TaskSpecSchema.task_id: 'distance_by_cupy',\n",
    "    TaskSpecSchema.node_type: 'CupyDistanceNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {},\n",
    "    TaskSpecSchema.inputs: {\n",
    "        'points_df_in': 'distributed_points.points_ddf_out'\n",
    "    }\n",
    "}\n",
    "\n",
    "verify_cudf_to_numba_tspec = {\n",
    "    TaskSpecSchema.task_id: 'verify_cudf_to_numba',\n",
    "    TaskSpecSchema.node_type: 'VerifyNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {\n",
    "        'df1_col': 'distance_cudf',\n",
    "        'df2_col': 'distance_numba'\n",
    "    },    \n",
    "    TaskSpecSchema.inputs: {\n",
    "        'df1': 'distance_by_cudf.distance_df',\n",
    "        'df2': 'distance_by_numba.distance_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "verify_cudf_to_cupy_tspec = {\n",
    "    TaskSpecSchema.task_id: 'verify_cudf_to_cupy',\n",
    "    TaskSpecSchema.node_type: 'VerifyNode',\n",
    "    TaskSpecSchema.filepath: get_file_path('notebooks/custom_port_nodes.py'),\n",
    "    TaskSpecSchema.conf: {\n",
    "        'df1_col': 'distance_cudf',\n",
    "        'df2_col': 'distance_cupy'\n",
    "    },    \n",
    "    TaskSpecSchema.inputs: {\n",
    "        'df1': 'distance_by_cudf.distance_df',\n",
    "        'df2': 'distance_by_cupy.distance_df'\n",
    "    }\n",
    "}\n",
    "\n",
    "task_list = [\n",
    "    points_tspec,\n",
    "    distribute_tspec,\n",
    "    dask_cudf_distance_tspec,\n",
    "    dask_numba_distance_tspec,\n",
    "    dask_cupy_distance_tspec,\n",
    "    verify_cudf_to_numba_tspec,\n",
    "    verify_cudf_to_cupy_tspec\n",
    "]\n",
    "\n",
    "task_graph = TaskGraph(task_list)\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD df_w_cudf:\n",
      "          x         y  distance_cudf\n",
      "0  0.979811  0.120089       0.987143\n",
      "1  0.933339  0.715111       1.175799\n",
      "2  0.347925  0.707020       0.787990\n",
      "3  0.201957  0.914520       0.936555\n",
      "4  0.819087  0.042769       0.820203\n",
      "\n",
      "HEAD df_w_numba:\n",
      "          x         y  distance_numba\n",
      "0  0.979811  0.120089        0.987143\n",
      "1  0.933339  0.715111        1.175799\n",
      "2  0.347925  0.707020        0.787990\n",
      "3  0.201957  0.914520        0.936555\n",
      "4  0.819087  0.042769        0.820203\n",
      "\n",
      "HEAD df_w_cupy:\n",
      "          x         y  distance_cupy\n",
      "0  0.979811  0.120089       0.987143\n",
      "1  0.933339  0.715111       1.175799\n",
      "2  0.347925  0.707020       0.787990\n",
      "3  0.201957  0.914520       0.936555\n",
      "4  0.819087  0.042769       0.820203\n",
      "\n",
      "Max Difference cudf to numba: 2.220446049250313e-16\n",
      "Max Difference cudf to cupy: 2.220446049250313e-16\n"
     ]
    }
   ],
   "source": [
    "out_list = [\n",
    "    'distance_by_cudf.distance_df',\n",
    "    'distance_by_numba.distance_df',\n",
    "    'distance_by_cupy.distance_df',\n",
    "    'verify_cudf_to_numba.max_diff',\n",
    "    'verify_cudf_to_cupy.max_diff'\n",
    "]\n",
    "\n",
    "(ddf_w_cudf, ddf_w_numba, ddf_w_cupy,\n",
    " mdiff_cudf_to_numba, mdiff_cudf_to_cupy) = task_graph.run(out_list)\n",
    "\n",
    "print('HEAD df_w_cudf:\\n{}\\n'.format(ddf_w_cudf.head()))\n",
    "print('HEAD df_w_numba:\\n{}\\n'.format(ddf_w_numba.head()))\n",
    "print('HEAD df_w_cupy:\\n{}\\n'.format(ddf_w_cupy.head()))\n",
    "print('Max Difference cudf to numba: {}'.format(mdiff_cudf_to_numba))\n",
    "print('Max Difference cudf to cupy: {}'.format(mdiff_cudf_to_cupy))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final illustration is how to save and load a task graph to a file for re-use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_graph.save_taskgraph('custom_wflow.gq.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gQuant TaskGraph file is created and saved. You can double click on it to open it up in the JupyterLab to edit it.\n",
    "\n",
    "Or you can display it by gQuant widget and play with it interactively "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25c1b23ac5964b1cb50df351c8462c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "GQuantWidget(sub=HBox(), value=[OrderedDict([('id', 'points_task'), ('type', 'PointNode'), ('conf', {'npts': 1…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "task_graph = TaskGraph.load_taskgraph('custom_wflow.gq.yaml')\n",
    "task_graph.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can run it by callign `run` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c228a8e4226a4fa0b2419bbcf9284dec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tab(children=(Output(), Output(), Output(), Output(), Output(), Output(layout=Layout(border='1px solid black')…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# update npartitions in case the scheduler is running with\n",
    "# different number of workers than what was saved.\n",
    "npartitions = len(client.scheduler_info()['workers'])\n",
    "replace_spec = {\n",
    "    'distributed_points': {\n",
    "        TaskSpecSchema.conf: {'npartitions': npartitions},\n",
    "    }\n",
    "}\n",
    "\n",
    "out_list = [\n",
    "    'distance_by_cudf.distance_df',\n",
    "    'distance_by_numba.distance_df',\n",
    "    'distance_by_cupy.distance_df',\n",
    "    'verify_cudf_to_numba.max_diff',\n",
    "    'verify_cudf_to_cupy.max_diff'\n",
    "]\n",
    "\n",
    "task_graph.run(out_list, replace=replace_spec, formated=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEAD df_w_cudf:\n",
      "          x         y  distance_cudf\n",
      "0  0.979811  0.120089       0.987143\n",
      "1  0.933339  0.715111       1.175799\n",
      "2  0.347925  0.707020       0.787990\n",
      "3  0.201957  0.914520       0.936555\n",
      "4  0.819087  0.042769       0.820203\n",
      "\n",
      "HEAD df_w_numba:\n",
      "          x         y  distance_numba\n",
      "0  0.979811  0.120089        0.987143\n",
      "1  0.933339  0.715111        1.175799\n",
      "2  0.347925  0.707020        0.787990\n",
      "3  0.201957  0.914520        0.936555\n",
      "4  0.819087  0.042769        0.820203\n",
      "\n",
      "HEAD df_w_cupy:\n",
      "          x         y  distance_cupy\n",
      "0  0.979811  0.120089       0.987143\n",
      "1  0.933339  0.715111       1.175799\n",
      "2  0.347925  0.707020       0.787990\n",
      "3  0.201957  0.914520       0.936555\n",
      "4  0.819087  0.042769       0.820203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('HEAD df_w_cudf:\\n{}\\n'.format(ddf_w_cudf.head()))\n",
    "print('HEAD df_w_numba:\\n{}\\n'.format(ddf_w_numba.head()))\n",
    "print('HEAD df_w_cupy:\\n{}\\n'.format(ddf_w_cupy.head()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "Using customized GPU kernels allows data scientists to implement and incorporate advanced algorithms. We demonstrated implementations using Numba and CuPy.\n",
    "\n",
    "The Numba approach enables data scientists to write GPU kernels directly in the Python language. Numba is easy to use for implementing and accelerating computations. However there is some overhead incurred for compiling the kernels whenever the Numba GPU kernels are used for the first time in a Python process. Currently Numba library only supports primitive data types. Some advanced CUDA programming features, such as function pointers and function recursions are not supported. \n",
    "\n",
    "The Cupy method is very flexible, because data scientists are writing C/C++ GPU kernels with CUDA directly. All the CUDA programming features are supported. CuPy compiles the kernel and caches the device code to the filesystem. The launch overhead is low. Also, the GPU kernel is built statically resulting in runtime efficiency. However it might be harder for data scientists to use, because C/C++ programming is more complicated. \n",
    "\n",
    "Below is a brief summary comparison table:\n",
    "\n",
    "| Methods | Development Difficulty | Flexibility | Efficiency | Latency |\n",
    "|---|---|---|---|---|\n",
    "| Numba method | medium | medium | low | high |\n",
    "| CuPy method | hard | high  | high | low |\n",
    "\n",
    "We recommend that the data scientists select the approach appropriate for their task taking into consideration the efficiency, latency, difficulty and flexibility of their workflow. \n",
    "\n",
    "In this blog, we showed how to wrap the customized GPU kernels in gQuant nodes. Also, by taking advantage of having the gQuant handle the low-level Dask interfaces for the developer, we demonstrated how to use the gQuant workflow with Dask distributed computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "\n",
    "# Shutdown the Dask cluster\n",
    "client.close()\n",
    "cluster.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'ok', 'restart': True}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "app = IPython.Application.instance()\n",
    "app.kernel.do_shutdown(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
